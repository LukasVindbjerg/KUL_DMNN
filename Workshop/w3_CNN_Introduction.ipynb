{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mGL-C5--oU7"
      },
      "source": [
        "# Credits\n",
        "\n",
        "Exercises are made with inspiration from the [DTU-deep-learning](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/4_Convolutional/4.1-CNN-Introduction.ipynb) and [PyTorch Tutorials](https://pytorch.org/tutorials/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXF54leN-oVA"
      },
      "source": [
        "# Introduction to Convolutional Neural Networks\n",
        "\n",
        "This exercise will introduce Convolutional Neural Networks in Pytorch. An important part of the exercise is to read and understand the PyTorch syntax and how, e.g., training loops are structured.\n",
        "You also have to implement your own CNN for classification :-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPxLUQ4M-oVC",
        "outputId": "b6e928f2-591b-4926-8968-ddf42a42616f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 71253601.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 50000\n",
              "    Root location: ./data\n",
              "    Split: Train"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Later in the exercise, you will have to download the CIFAR-10 data-set. It might take a while,\n",
        "# so it would be a good idea to run this cell to start downloading the data already now.\n",
        "import torchvision\n",
        "torchvision.datasets.CIFAR10(root='./data', download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_vvOrtz-oVF"
      },
      "source": [
        "# Exercise 1:\n",
        "\n",
        "### Exercise 1.1: Convolve manually\n",
        "\n",
        "Perform the following calculations by hand, and write the result below. You deside if you want do make a convoliution or correlation.\n",
        "\n",
        "\n",
        "<img src=\"https://nextcloud.theailab.dk/s/HmPSbLNtzQFDfbG/preview\" alt=\"drawing\" style=\"width:400px;\"/>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. Manually convolve the input with the kernel, and write down the result. Use no padding and stride of 1.\n",
        "1. Perform $2\\times2$ max pooling on the output of the convolution. Use a stride of 2.\n",
        "\n",
        "**Answer:**\n",
        "Performing correlation yields the following\n",
        "\n",
        "\\begin{array}{|c|c|}\n",
        "\\hline\n",
        "     21 & 9 \\\\\\hline\n",
        "    \\color{red}{29}& 20 \\\\\\hline\n",
        "\\end{array}\n",
        "\n",
        "Max pooling gives us 29\n",
        "\n",
        "Performing convolution yields the following\n",
        "\n",
        "\\begin{array}{|c|c|}\n",
        "\\hline\n",
        "     2 & 0 \\\\\\hline\n",
        "    \\color{red}{9}& 0 \\\\\\hline\n",
        "\\end{array}\n",
        "\n",
        "Max pooling gives us 9\n",
        "\n",
        "### Exercise 1.2: Calculate output sizes of convolution output\n",
        "\n",
        "In the following list, you will be given a 3D tensor and some filters. Based on their sizes, calculate the size of the output (if valid). We will use PyTorch notation of dimensions. This means that dimensions are given as channel-first. I.e.: `(channel, height, width)`.\n",
        "\n",
        "The size of the 3D tensor input is givens as `(channel, height, width)`.\n",
        "A number `(channels_out)` of filters, have a size of `(channels_in, filter_height, filter_width)`, stride `(height, width)` and padding `(height, width)`.\n",
        "\n",
        "\n",
        "1. input tensor with dimensionality (1, 28, 28) and 16 filters of size (1, 5, 5) with stride (1, 1) and padding (0, 0)\n",
        "\n",
        "    **Answer:** (16, 24, 24)\n",
        "\n",
        "2. input tensor with dimensionality (3, 32, 32) and 24 filters of size (3, 3, 3) with stride (1, 1) and padding (0, 0)\n",
        "\n",
        "    **Answer:** (24, 30, 30)\n",
        "\n",
        "3. input tensor with dimensionality (40, 32, 32) and 3 filters of size (40, 2, 2) with stride (2, 2) and padding (0, 0)\n",
        "\n",
        "     **Answer:** (3, 16, 16)\n",
        "\n",
        "4. input tensor with dimensionality (11, 8, 16) and 7 filters of size (11, 3, 3) with stride (2, 2) and padding (1, 1)\n",
        "\n",
        "    **Answer:** (7, **7//2** + 1, **15//2** + 1 ) = (7, 4, 8)\n",
        "\n",
        "5. input tensor with dimensionality (128, 256, 256) and 112 filters of size (128, 3, 3) with stride (1, 1) and padding (1, 1)\n",
        "\n",
        "    **Answer:** (112, 256, 256)\n",
        "\n",
        "### Exercise 1.3: Associative property of convolution\n",
        "Assume that we have two convolution kernels of size k1 and k2, respectively (with no nonlinearity in-between). The kernels are applied one after the other.Note that to prove associativity you will have to use convolution with the \"full\" padding scheme, not \"valid\" or \"same\". As a practical matter, use the definition of 2d convolution from the slides.\n",
        "\n",
        "  1. **Prove that the result of the operation can be expressed by a single convolution.**\n",
        "  \n",
        "  Let $X$ be the input data and $K_1, K_2$ the kernels. We will restrict ourselves to odd sizes $k_1, k_2$ and denote $d_1 = \\frac{k_1 - 1}{2}$ and similarly $d_2 = \\frac{k_2 - 1}{2}$. For convenience let\n",
        "  \\begin{equation}\n",
        "  Z(x,y) = X \\ast K_1 (x,y) =  \\sum_{s_1 = -d_1}^{d_1} \\sum_{t_1 = -d_1}^{d_1} X(x-s_1, y-t_1) K_1(s_1, t_1)\n",
        "  \\end{equation}\n",
        "  We can then write\n",
        "  \\begin{align}\n",
        "  (X \\ast K_1) \\ast K_2 (x,y) &= \\sum_{s_2 = -d_2}^{d_2} \\sum_{t_2 = -d_2}^{d_2} Z(x-s_2, y-t_2) K_2(s_2, t_2) \\\\\n",
        "     &= \\sum_{s_2, t_2} \\left( \\sum_{s_1, t_1} X(x-s_2 - s_1, y-t_2 - t_1) K_1(s_1,t_1)  \\right) K_2(s_2, t_2) \\\\\n",
        "     &= \\sum_{s_2, t_2} \\sum_{s_1, t_1} X(x-s_2 -s_1, y-t_2 - t_1) K_1(s_1,t_1) K_2(s_2, t_2) \\\\\n",
        "  \\end{align}\n",
        "  The input factor doesn't depend explicitly on the tuple $(s_1,t_1, s_2, t_2)$ but rather on the values $\\alpha = s_1 + s_2$ and $\\beta = t_1 + t_2$ i.e. $X(x-s_2 - s_1, y-t_2 - t_1) = X(x-\\alpha, y- \\beta)$ so we can write the sum as\n",
        "    \\begin{align}\n",
        "  (X \\ast K_1) \\ast K_2 (x,y)\n",
        "  &= \\sum_{\\alpha = -(d_1+d_2)}^{d_1+d_2}\\sum_{\\beta = -(d_1+d_2)}^{d_1+d_2}  X(x-\\alpha, y- \\beta) \\left(\\sum_{s_1 + s_2 = \\alpha \\\\ t_1 + t_2 = \\beta} K_1(s_1,s_2) K_2(s_2, t_2)\\right) \\\\\n",
        "  &= \\sum_{\\alpha = -(d_1 + d_2)}^{d_1 + d_2}\\sum_{\\beta = -(d_1 + d_2)}^{d_1 + d_2}  X(x-\\alpha, y- \\beta) K(\\alpha, \\beta) \\\\\n",
        "   &= (X \\ast K) (x,y) \\> .\n",
        "  \\end{align}\n",
        "\n",
        "  So the convolution with $K_1$ and $K_2$ is equivalent to convolution with some $K$. We can show that this proves the associativity of the convolution by considering $K(\\alpha, \\beta)$ more carefully\n",
        "  \\begin{align}\n",
        "  K(\\alpha, \\beta) &= \\sum_{s_1 + s_2 = \\alpha \\\\ t_1 + t_2 = \\beta} K_1(s_1,t_1) K_2(s_2, t_2) \\\\\n",
        "                  & = \\sum_{s_2, t_2} K_1(\\alpha - s_2, \\beta - t_2) K_2(s_2, t_2) \\\\\n",
        "                  &= (K_1 \\ast K_2) (\\alpha, \\beta)\n",
        "  \\end{align}\n",
        "  and so we have shown associativity i.e.\n",
        "  \\begin{equation}\n",
        "      (X \\ast K_1 ) \\ast K_2 = X \\ast (K_1 \\ast K_2)\n",
        "  \\end{equation}\n",
        "  \n",
        "  In the case of an even kernel size the sum would no longer be symmetric around zero e.g. going from $-d_1,\\cdots,d_1$, but this won't change the idea of the above proof in any fundamental way.\n",
        "\n",
        " Note that this is not convolution as it is implemented in pytorch's Conv2d, and so associativity is **not** guaranteed for pytorch tensors when convolving. Using _scipy.signal.convolve2d_ with _mode=\"full\"_ will work as described in this section and be associative.\n",
        "\n",
        "  \n",
        "  2. **What is the dimensionality of the equivalent single convolution?**\n",
        "  \n",
        "  Consider the sum of the equivalent convolution, where the indices $\\alpha, \\beta$ run from  $-(d_1 + d_2)$ to $d_1 + d_2$ corresponding to all the possible combinations of e.g. $s_1$ and $s_2$. This means that the equivalent kernel size $k$ is\n",
        "    \\begin{equation}\n",
        "    k = 2(d_1 + d_2) + 1 = 2 \\left( \\frac{k_1-1}{2} + \\frac{k_2-1}{2} \\right) + 1 = k_1 + k_2 -1\n",
        "    \\end{equation}\n",
        "  \n",
        "  \n",
        "  3. **Is the converse true, i.e., can you always decompose a convolution into two smaller ones?**\n",
        "  \n",
        "  In a strict sense, **no**. Proof by technicality: If the kernel size 1x1 there does not exist smaller kernels\n",
        "\n",
        "We will give some less trivial examples:\n",
        "  \n",
        "  For size 2 kernels the answer is also no: consider the equation $k=k_1 + k_2 - 1$ from above, in order to get $k = 2$ at least one of $k_1$ and $k_2$ would have to be $2$ i.e. not smaller.\n",
        "  \n",
        "  For size $\\geq 3$ kernels the dimensions allows for such decomposition e.g. $k_1 = k_2 = 2$ means $k=3$. But using $2(3-1)^2=8$ components we cannot map to all $3^2=9$ component matrices. Thus simple linear algebra does not allow for all 3x3 kernels to be decompositioned into the convolution of two smaller kernels.\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiQQ0PHa-oVG"
      },
      "source": [
        "# Exercise 2: Creating a convolutional neural network\n",
        "\n",
        "In this exercise, we will once again train a network to recognize MNIST digits.\n",
        "First, we will set up the data and the network. Please spend some time reading and understanding the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksx-o9Bn-oVH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJGldiK9-oVJ"
      },
      "source": [
        "## Data: MNIST\n",
        "\n",
        "This code prepares MNIST and sets dimensions to `(num_samples, num_channels, height, width)`, as PyTorch expects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs6AVjOb-oVK"
      },
      "outputs": [],
      "source": [
        "from mlxtend.data import mnist_data\n",
        "X, y = mnist_data()\n",
        "X = (X/255).astype(np.float32) # Convert to interval 0:1\n",
        "y = y.astype(np.float32)\n",
        "num_classes = 10\n",
        "nchannels, rows, cols = 1, 28, 28\n",
        "\n",
        "X = np.expand_dims(X.reshape(len(X),rows,cols),1) # Add a channel-dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KGH9fKi-oVK"
      },
      "source": [
        "Split data in train, validation, and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIhbTUDx-oVM"
      },
      "outputs": [],
      "source": [
        "def splitdata(X, y, Ngroups, weights=None):\n",
        "    '''\n",
        "    X, y = input data and labels\n",
        "    Ngroups = number of groups to split data into\n",
        "    weights = a list with Ngroups weights, that tell the probability of a sample ending in either of the data sets\n",
        "    '''\n",
        "    if weights is None:\n",
        "        weights = [1/Ngroups]*Ngroups\n",
        "\n",
        "    groups = np.array(random.choices(list(range(Ngroups)), weights=weights, cum_weights=None, k=len(X)))\n",
        "    return ((X[np.where(groups==g)], y[np.where(groups==g)]) for g in np.array(range(Ngroups)))\n",
        "\n",
        "(x_train, targets_train), (x_valid, targets_valid), (x_test, targets_test) = splitdata(X, y, Ngroups=3, weights=[0.8, 0.1, 0.1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MxC0x7l-oVN"
      },
      "source": [
        "## Let's plot some samples from the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek-ps6hd-oVN",
        "outputId": "296b2d61-9230-43be-c0e7-27ab5ac68a68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 12 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "fig = plt.figure()\n",
        "nrows_ncols=(1, 6)\n",
        "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
        "                 nrows_ncols=nrows_ncols,  # creates 2x2 grid of axes\n",
        "                 axes_pad=0.1,  # pad between axes in inch.\n",
        "                 )\n",
        "\n",
        "for ax, idx in zip(grid, np.random.randint(low=0, high=len(x_train), size=np.prod(nrows_ncols))):\n",
        "    # Iterating over the grid returns the Axes.\n",
        "    ax.imshow(X[idx].reshape(28,28), cmap='gray')\n",
        "    ax.set_title(str(int(y[idx])))\n",
        "    ax.set_axis_off()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEBMGM1m-oVO"
      },
      "source": [
        "# Define a simple feed forward neural network without convolutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "un5Ba6Zk-oVO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "\n",
        "from torch.nn import Linear, Conv2d, BatchNorm2d, MaxPool2d, Dropout2d\n",
        "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR3mhgQH-oVP"
      },
      "source": [
        "We'll use the nn.Module class to define our network. It will help us create clearer and more compact code by encapsulating the network in a class. It also helps us, if we want to save our trained weights and load them again.\n",
        "\n",
        "Everything that contains weights, which you want to be trained during the training process should be defined in your \\_\\_init\\_\\_ method.\n",
        "\n",
        "You can read more about the nn.Module class here:\n",
        "https://pytorch.org/tutorials/beginner/nn_tutorial.html"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "83PT-55d-oVP"
      },
      "source": [
        "# hyperameters of the model\n",
        "num_classes = 10\n",
        "channels = x_train.shape[1]\n",
        "height = x_train.shape[2]\n",
        "width = x_train.shape[3]\n",
        "\n",
        "\n",
        "# define network\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        # YOUR CODE BEGINS HERE\n",
        "        # Define the needed Conv-layer(s)\n",
        "        # YOUR CODE ENDS HERE\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.FC1 = nn.Linear(height*width, 512)\n",
        "        self.FC2 = nn.Linear(512, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # YOUR CODE BEGINS HERE\n",
        "        # Define your network architecture\n",
        "        x = self.flatten(x)\n",
        "        x = self.FC1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.FC2(x)\n",
        "        # YOUR CODE ENDS HERE\n",
        "        return x\n",
        "\n",
        "net = NeuralNetwork()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekg5SYDXdftE",
        "outputId": "b817bda1-51c9-41d0-a9a4-10c131aa5e80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "  (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (FC1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (FC2): Linear(in_features=512, out_features=10, bias=True)\n",
            "  (FC3): Linear(in_features=6272, out_features=10, bias=True)\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#SOLUTION\n",
        "# hyperameters of the model\n",
        "num_classes = 10\n",
        "channels = x_train.shape[1]\n",
        "height = x_train.shape[2]\n",
        "width = x_train.shape[3]\n",
        "\n",
        "\n",
        "# define network\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding='same')\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding='same')\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding='same')\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.FC1 = nn.Linear(height*width, 512)\n",
        "        self.FC2 = nn.Linear(512, num_classes)\n",
        "        self.FC3 = nn.Linear(14*14*32, num_classes)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        logits = self.FC3(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "net = NeuralNetwork()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ogb2gRsS-oVQ"
      },
      "source": [
        "## Setting up loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3mx6HDq-oVQ"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaRph9wS-oVR"
      },
      "source": [
        "We can test our architecture by forwarding random data through it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwXMO0wP-oVR",
        "outputId": "f0a08525-3441-4d86-b762-527cdbbeee8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([3, 10])\n",
            "Output tensor:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0253, -0.0641, -0.0649,  0.0094,  0.0368,  0.0043, -0.0425,  0.0046,\n",
              "          0.0270,  0.0003],\n",
              "        [-0.0074, -0.0765, -0.0499, -0.0264,  0.0004,  0.0222, -0.0314, -0.0393,\n",
              "          0.0122, -0.0134],\n",
              "        [-0.0098, -0.0509,  0.0160,  0.0223,  0.0139, -0.0160, -0.0341, -0.0348,\n",
              "          0.0024, -0.0042]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "x = np.random.randn(3, 1, 28, 28).astype('float32')\n",
        "out = net(torch.from_numpy(x))\n",
        "print('Output shape:', out.shape)\n",
        "print('Output tensor:')\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP7XGBSh-oVR"
      },
      "source": [
        "Notice how the default weight initialization distributes the output scores nicely between the classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUTo1QMC-oVS"
      },
      "source": [
        "## Now we set up the training loop.\n",
        "Notice that we don't use dataloaders, since writing the training loop manually gives a better understanding of what is going on.\n",
        "Please spend some time going through the following code-block line-by-line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXDRwyqY-oVS",
        "outputId": "5bfe54b7-77a3-4348-c6d8-66187e086526",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1 : Train Loss 0.922545 , Train acc 0.095250, Valid acc 0.096000\n",
            "Epoch 11 : Train Loss 0.921097 , Train acc 0.103750, Valid acc 0.104000\n",
            "Epoch 21 : Train Loss 0.919557 , Train acc 0.119250, Valid acc 0.136000\n",
            "Epoch 31 : Train Loss 0.917530 , Train acc 0.183250, Valid acc 0.180000\n",
            "Epoch 41 : Train Loss 0.914324 , Train acc 0.246250, Valid acc 0.238000\n",
            "Epoch 51 : Train Loss 0.908206 , Train acc 0.317750, Valid acc 0.318000\n",
            "Epoch 61 : Train Loss 0.893009 , Train acc 0.407000, Valid acc 0.408000\n",
            "Epoch 71 : Train Loss 0.837124 , Train acc 0.540000, Valid acc 0.520000\n",
            "Epoch 81 : Train Loss 0.505689 , Train acc 0.630250, Valid acc 0.600000\n",
            "Epoch 91 : Train Loss 0.235428 , Train acc 0.720000, Valid acc 0.692000\n"
          ]
        }
      ],
      "source": [
        "batch_size = 100\n",
        "num_epochs = 100\n",
        "num_samples_train = x_train.shape[0]\n",
        "num_batches_train = num_samples_train // batch_size\n",
        "num_samples_valid = x_valid.shape[0]\n",
        "num_batches_valid = num_samples_valid // batch_size\n",
        "\n",
        "train_acc, train_loss = [], []\n",
        "valid_acc, valid_loss = [], []\n",
        "test_acc, test_loss = [], []\n",
        "cur_loss = 0\n",
        "losses = []\n",
        "\n",
        "def get_slice(i, size):\n",
        "    return range(i * size, (i + 1) * size)\n",
        "\n",
        "def accuracy_score(targets, predictions):\n",
        "    return 1.0*np.sum((np.array(targets) == np.array(predictions)))/len(targets)\n",
        "\n",
        "# This function iterates through all batches of a dataset, X_data and returns predictions and matching targets\n",
        "def evaluateLoop(X_data, targets, num_batches):\n",
        "    allTargets, allPredictions = [], []\n",
        "\n",
        "    #Iterate through the batches\n",
        "    for i in range(num_batches):\n",
        "        slce = get_slice(i, batch_size)\n",
        "        x_batch = torch.from_numpy(X_data[slce])\n",
        "\n",
        "        output = net(x_batch)\n",
        "        predictions = torch.max(output, 1)[1]\n",
        "\n",
        "        allTargets += list(targets[slce])\n",
        "        allPredictions += list(predictions.data.numpy())\n",
        "\n",
        "    return allTargets, allPredictions\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    ## Train\n",
        "    cur_loss = 0\n",
        "    net.train()\n",
        "\n",
        "    for i in range(num_batches_train):\n",
        "        slce = get_slice(i, batch_size)\n",
        "        x_batch = torch.from_numpy(x_train[slce])\n",
        "        output = net(x_batch) # Forward pass\n",
        "\n",
        "        # compute gradients given loss\n",
        "        target_batch = torch.from_numpy(targets_train[slce]).long()\n",
        "        batch_loss = criterion(output, target_batch) # Calculate loss\n",
        "        optimizer.zero_grad() # Reset gradients\n",
        "        batch_loss.backward() # Backpropagation\n",
        "        optimizer.step() # Update parameters\n",
        "\n",
        "        cur_loss += batch_loss\n",
        "    losses.append(cur_loss / batch_size)\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    ### Evaluate training data\n",
        "    train_targs, train_preds = evaluateLoop(X_data=x_train, targets=targets_train, num_batches=num_batches_train)\n",
        "\n",
        "\n",
        "    ### Evaluate validation data\n",
        "    val_targs, val_preds = evaluateLoop(X_data=x_valid, targets=targets_valid, num_batches=num_batches_valid)\n",
        "\n",
        "\n",
        "    #Calculate accuracies\n",
        "    train_acc_cur = accuracy_score(train_targs, train_preds)\n",
        "    valid_acc_cur = accuracy_score(val_targs, val_preds)\n",
        "\n",
        "    train_acc.append(train_acc_cur)\n",
        "    valid_acc.append(valid_acc_cur)\n",
        "\n",
        "    #Plot output\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (\n",
        "                epoch+1, losses[-1], train_acc_cur, valid_acc_cur))\n",
        "\n",
        "### Evaluate test set\n",
        "x_batch = torch.from_numpy(x_test)\n",
        "output = net(x_batch)\n",
        "preds = torch.max(output, 1)[1]\n",
        "print(\"\\nTest set Acc:  %f\" % (accuracy_score(list(targets_test), list(preds.data.numpy()))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpMEMbNk-oVS"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation accuracies vs. epochs\n",
        "epoch = range(len(train_acc))\n",
        "plt.figure()\n",
        "plt.plot(epoch, train_acc, 'r', epoch, valid_acc, 'b')\n",
        "plt.legend(['Train Accuracy', 'Val Accuracy'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHPVvGiA-oVT"
      },
      "source": [
        "**Exercise**\n",
        "\n",
        "1. Replace the first fully connected layer with a 2D convolution layer and adjust the input size of the remaining fully connected layer so that it fits the new input. Does the performance change?\n",
        "(try num_filters=16 and filter_size=5 as a starting point).\n",
        "You can define a conv-layers using the following code:\n",
        "\n",
        "```\n",
        "self.conv1 = nn.Conv2d(in_channels=<in_channels>, out_channels=<out_channels>, kernel_size=<kernel_size>)\n",
        "```\n",
        "\n",
        "2. After the convolution layer, the size of the input is reduced by `(heigh-1,width-1)` pixels, where `(height,width)` is the filter size. This limits the number of convolutional layers that can be stacked.\n",
        "Try adding adequate padding to the convolutional layer to retain the spatial dimensions.\n",
        "\n",
        "3. Try adding more convolution layers and see if you can increase the performance.\n",
        "\n",
        "4. Increasing the number of convolutions also increases the number of calculations. Try adding a $2\\times 2$ max-pool layer after your convolution to decrease the size of the feature-map, while keeping the important features.\n",
        "\n",
        "    You can define a pooling layer using the following code:\n",
        "\n",
        "```\n",
        "self.pool = nn.MaxPool2d(kernel_size=<kernel_size>, stride=<stride>)\n",
        "```\n",
        "\n",
        "5. Try experimenting with more convolutional layers and pooling layers and see how it influences the performance.\n",
        "\n",
        "\n",
        "  <pre>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aihme2io-oVT"
      },
      "source": [
        "# Let's try using RGB images\n",
        "## Data: CIFAR-10\n",
        "\n",
        "The CIFAR-10 dataset (Canadian Institute For Advanced Research) is a collection of images that are commonly used to train machine learning and computer vision algorithms.\n",
        "You can see it as a \"Color-MNIST\" dataset.\n",
        "\n",
        "The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class. (https://en.wikipedia.org/wiki/CIFAR-10)\n",
        "\n",
        "The following code will set up a dataloader that processes images and create mini-batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqTzwYxW-oVT"
      },
      "outputs": [],
      "source": [
        "# Load CIFAR-10 and set-up dataloaders. PyTorch has helper-functions for downloading the dataset the first time it is used.\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Transforms are common image transformations, that can be stacked and used for preprocessing images.\n",
        "# Here, our preprocessing consists of converting the data to torch tensors and normalizing the data\n",
        "# with 0.5 mean and 0.5 std. diviation for all three channels\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "\n",
        "# Set up the training set. The data-set helper function already implemented a data-split\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "# Set up the test set. The data-set helper function already implemented a data-split\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# The CIFAR-10 Classes\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4dwgjMq-oVU"
      },
      "source": [
        "## Data visalization\n",
        "Let's visualize some of the images from CIFAR-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05iVZnJg-oVU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urQ4RtDY-oVV"
      },
      "source": [
        "## Exercise 2.1: Define a simple neural network\n",
        "\n",
        "\n",
        "\n",
        "__Exercise__: Define a convolutional neural network. Start out with a small network and test that everything is working. Then start adding more layers. Remember that the output should be a vector with 10 indices.\n",
        "Try adding both conv-layers, pooling layers, fully-connected layers and activations\n",
        "\n",
        "A reasonable first architecture could be:\n",
        "\n",
        "| Layer type   | size    | Output channels  | Stride |\n",
        "|--------------|---------|------------------|--------|\n",
        "| conv         | 5x5     | 6                | 1      |\n",
        "| pool         | 2x2     |                  | 2      |\n",
        "| conv         | 5x5     | 16               | 1      |\n",
        "| fully connected   |      | 120               |       |\n",
        "| fully connected   |      | 184               |       |\n",
        "| fully connected   |      | 10               |       |\n",
        "\n",
        "\n",
        "\n",
        "You can define a conv-layers and pooling layers using the following code:\n",
        "\n",
        "```\n",
        "self.conv1 = nn.Conv2d(in_channels=<in_channels>, out_channels=<out_channels>, kernel_size=<kernel_size>)\n",
        "self.pool = nn.MaxPool2d(kernel_size=<kernel_size>, stride=<stride>)\n",
        "```\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "wTKe-6nW-oVV"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        ## YOUR CODE GOES HERE ##\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ## YOUR CODE GOES HERE ##\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "MYsletFodftL"
      },
      "source": [
        "#SOLUTION\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(in_features=16 * 5 * 5, out_features=120)\n",
        "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "        self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkttE0KcdftL"
      },
      "outputs": [],
      "source": [
        "#Solution with Global Average pooling\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(in_features=16 * 5 * 5, out_features=120)\n",
        "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "        self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
        "        self.fc4 = nn.Linear(in_features=10, out_features=10)\n",
        "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=10, kernel_size=5)\n",
        "        self.globalAvgPooling = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.globalAvgPooling(x)\n",
        "\n",
        "        #x = self.fc4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrBCQQd5-oVW"
      },
      "source": [
        "## Defining a loss-function\n",
        "\n",
        "Let’s use a Classification Cross-Entropy loss and SGD with momentum.\n",
        "Our labels are one-hot encoded, which means that for each image, we have a target vector with 10 entries (one for each class).\n",
        "This target vector contains only zeros except for the entry of the class to which the image belongs.\n",
        "\n",
        "\\begin{equation}\n",
        "H(y,\\hat y) = -\\sum_j y_j \\cdot log(\\hat y_j)\n",
        "\\end{equation}\n",
        "\n",
        "where\n",
        "\n",
        "$j=(1,2,...,n_{classes})$ and $\\hat y=\\begin{cases}\n",
        "      1 & j=\\text{true class}\\\\\n",
        "      0 & \\text{otherwise}\n",
        "    \\end{cases}     $\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ei54jAWZ-oVW"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # Let's use stocastic gradient descent (We will talk about momentum next week)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36KRYYot-oVW"
      },
      "source": [
        "## Train the network\n",
        "\n",
        "This is when things start to get interesting. We simply have to loop over our data iterator, and feed the inputs to the network and optimize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFDqoZL6-oVW"
      },
      "outputs": [],
      "source": [
        "nEpocs = 2 # How many times should we run over the data set?\n",
        "\n",
        "for epoch in range(nEpocs):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('Epoch %d, iteration %5d, Train loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMU5zZ0F-oVX"
      },
      "source": [
        "## Save the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W6G9Ti8-oVX"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QOJ6ZnD-oVX"
      },
      "source": [
        "## Test the network on the test data\n",
        "\n",
        "We have trained the network for 2 passes over the training dataset. But we need to check if the network has learnt anything at all.\n",
        "\n",
        "We will check this by predicting the class label that the neural network outputs, and checking it against the ground-truth. If the prediction is correct, we add the sample to the list of correct predictions.\n",
        "\n",
        "Okay, first step. Let us display an image from the test set to get familiar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zCLxjeG-oVX"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(testloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6MYS7hB-oVY"
      },
      "source": [
        "## Loading the model\n",
        "\n",
        "Let's load the model, we just saved. Since the weights are already set, we don't need this step, but in the future, you can skip the training pass, and just load the weights, you just trained. This is also used when doing _transfer learning_, which will be covered in a future lecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4BxnfOw-oVY"
      },
      "outputs": [],
      "source": [
        "net = Net()\n",
        "net.load_state_dict(torch.load(MODEL_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJW1Gh27-oVY"
      },
      "source": [
        "## Test the model\n",
        "Let's classify the images, we just displayed above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5naZyD2E-oVZ"
      },
      "outputs": [],
      "source": [
        "outputs = net(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMlKt9Vu-oVZ"
      },
      "source": [
        "The output of the network is the raw scores for the 10 classes. In the current network, these scores are unbounded. The higher the score for a class, the more the network thinks that the image is of the particular class. So, let’s get the index of the highest score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHX7i37e-oVa"
      },
      "outputs": [],
      "source": [
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
        "                              for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3HFAgiZ-oVa"
      },
      "source": [
        "## How was the results?\n",
        "Let's iterate through all the test images and see the performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CORiH6Am-oVa"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZDulHIf-oVb"
      },
      "source": [
        "# Global average pooling\n",
        "\n",
        "Recall that global average pooling takes a `(height x width x channels_in)` volume and reduces it to a `channels_in`-dimensional vector by averaging the `height x width` values of each channel.\n",
        "\n",
        "Thus, we can replace the flatten layer and the first fully connected layer with a `(1 x 1 x channels_out)` convolution, followed by a global average pooling layer.\n",
        "The cool thing about global average pooling is that it works for all input sizes. If you consider the\n",
        "\n",
        "**Exercise**\n",
        "Your task is to replace your flatten layer and first final fully connected layer with global average pooling. In PyTorch, this is called\n",
        "`nn.AdaptiveAvgPool2d`\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html\n",
        "\n",
        "The output of `AdaptiveAvgPool` is still a 3D volume `(1 x 1 x channels_out)`, so you have to reshape it. You can use `x = torch.flatten(x, 1)` after `AdaptiveAvgPool` to convert the output to a 1D vector for each sample.\n",
        "\n",
        "Now try inputting an image larger than 32x32 pixels. Even though the network accepts large images, it does not mean that it classifies them correctly. That is because the convolution operator is not scale invariant and, therefore, needs object of same size as in the training data.\n",
        "\n",
        "You can input your own image using this code:\n",
        "```\n",
        "import imageio\n",
        "im = imageio.imread('horse.jpg').astype(np.float32)\n",
        "im = torch.from_numpy(im).permute(2,0,1).unsqueeze(0)\n",
        "output = net(im)\n",
        "print(classes[np.argmax(output.detach().numpy())])\n",
        "```\n",
        "\n",
        "**Question** The network can only handle input images down to a certain shape (height/width).\n",
        "\n",
        " 1. Why?\n",
        " _The size of the feature decreases for each conv-layer that does not use padding and for each pooling-layer. If the input is too small, it cannot be reduced through these layers_\n",
        " 1. What is the smallest input width/height your model can handle?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SASPXaw9dftP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import imageio\n",
        "import torch\n",
        "im = imageio.imread('horse.jpg').astype(np.float32)\n",
        "im = torch.from_numpy(im).permute(2,0,1).unsqueeze(0)\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Resize((32,32)),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "im = transform(im)\n",
        "output = net(im)\n",
        "print(classes[np.argmax(output.detach().numpy())])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9muknP8r-oVc"
      },
      "source": [
        "# EXTRA: Train on the GPU\n",
        "\n",
        "If you have a CUDA-enabled computer you can try training the network on the GPU to see a great speed-up. You have to move your network and your variables to the GPU's memory, which you can do by adding the following lines to your code:\n",
        "```\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "```\n",
        "\n",
        "Remember that you will have to send the inputs and targets at every step to the GPU too:\n",
        "```\n",
        "inputs.to(device)\n",
        "labels.to(device)\n",
        "```\n",
        "\n",
        "When pulling data from the GPU, use the following commands:\n",
        "```\n",
        "myVar.cpu().detach().numpy()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFX8ovCX-oVc"
      },
      "source": [
        "# EXTRA: Use your own data\n",
        "Try training a network on your own classes, e.g., cats and dogs. You can use the following library to scrape images from google image search.\n",
        "https://pypi.org/project/simple-image-download/\n",
        "\n",
        "A good starting point is to use the `ImageFolder` dataset, which lets you define an image dataset as folders with names that indicate labels.\n",
        "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "\n",
        "You might also have to modify the `transformer`, which preprocesses the images. So far we have only used it to normalize images and convert them to tensors, but it can also be used to resize images (and much more):\n",
        "```\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Resize((32,32)),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]).\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zzuBOhh-oVd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}