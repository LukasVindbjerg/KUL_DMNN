{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i1KfYGFRRWP"
      },
      "source": [
        "#DATA MINING AND NEURAL NETWORKS    \n",
        "##Assignment 3.4 - Self-attention and Transformers\n",
        "\n",
        "In this file, we first understand the self-attention mechanism by implementing it both with ``NumPy`` and ``PyTorch``.\n",
        "Then, we implement a 6-layer Vision Transformer (ViT) and train it on the MNIST dataset.\n",
        "\n",
        "All training will be conducted on a single T4 GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlTJbgaaRTct",
        "outputId": "c301c82e-757b-4695-f37f-d8ac173bc95e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Please first load your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kokAQZSXRZ_C",
        "outputId": "f67953de-aaa8-44fd-d728-9b3c192c806f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: cd: /content/drive/MyDrive/DMNN/DMNN2023: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Please go to the folder with all assignment files\n",
        "# Please change the following path to your own path\n",
        "!cd /content/drive/MyDrive/DMNN/DMNN2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu6w5GLkRezN",
        "outputId": "49f0fff3-bd75-4098-b5b3-db5c4037ce6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Dec 22 13:19:44 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Please go to Edit > Notebook settings > Hardware accelerator > choose \"T4 GPU\"\n",
        "# Now check if you have loaded the GPU successfully\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-sUz1A9SzVH"
      },
      "source": [
        "# Self-attention Mechanism\n",
        "Self-attention is the core mechanism in Transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6ol1XZtiPpk"
      },
      "source": [
        "## Self-attention with NumPy\n",
        "To have a better understanding of it, we first manually implement self-attention mechanism with ``numpy``. You can check the dimension of each variable during the matrix computation.\n",
        "\n",
        "Feel free to change the dimensions of each variable and see how the output dimension will change accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgWIgp51RgC3",
        "outputId": "b8c838cd-892b-445c-be0a-61b736603288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The attention outputs are\n",
            " [[ 0.29813141 -0.5775645  -4.69297744 ...  2.21494554 -0.78305058\n",
            "   3.10710804]\n",
            " [ 0.29813141 -0.5775645  -4.69297744 ...  2.21494554 -0.78305058\n",
            "   3.10710804]\n",
            " [ 0.29813141 -0.5775645  -4.69297744 ...  2.21494554 -0.78305058\n",
            "   3.10710804]\n",
            " ...\n",
            " [ 0.29813141 -0.5775645  -4.69297744 ...  2.21494554 -0.78305058\n",
            "   3.10710804]\n",
            " [ 0.29813141 -0.5775645  -4.69297744 ...  2.21494554 -0.78305058\n",
            "   3.10710804]\n",
            " [ 0.29813141 -0.5775645  -4.69297744 ...  2.21494554 -0.78305058\n",
            "   3.10710804]]\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from numpy.random import randn\n",
        "\n",
        "# I. Define the input data X\n",
        "# X is of 32 samples, each sample of dimension 256\n",
        "d = 256\n",
        "n = 32\n",
        "X = randn(n, d) # (32, 256)\n",
        "\n",
        "# II. Generate the projection weights\n",
        "Wq = randn(d, d) # (256, 256)\n",
        "Wk = randn(d, d)\n",
        "Wv = randn(d, d)\n",
        "\n",
        "# III. Project X to queries, keys and values\n",
        "# We would train these in real life.\n",
        "Q = np.dot(X, Wq) # (32, 256)\n",
        "K = np.dot(X, Wk)\n",
        "V = np.dot(X, Wv)\n",
        "\n",
        "# IV. Compute the self-attention score, denoted by A\n",
        "# A = softmax(QK^T / \\sqrt{d})\n",
        "# Define the softmax function\n",
        "def softmax(z):\n",
        "    z = np.clip(z, 100, -100) # clip in case softmax explode\n",
        "    tmp = np.exp(z)\n",
        "    res = np.exp(z) / np.sum(tmp, axis=1)\n",
        "    return res\n",
        "\n",
        "# This represents how all the different samples are related to one another.\n",
        "A = softmax(np.dot(Q, K.transpose())/math.sqrt(d)) #(32, 32)\n",
        "\n",
        "# V. Compute the self-attention output\n",
        "# outputs = A * V\n",
        "outputs = np.dot(A, V) #(32, 256)\n",
        "\n",
        "print(\"The attention outputs are\\n {}\".format(outputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iozM1k4khO0B"
      },
      "source": [
        "## Self-attention with PyTorch\n",
        "Now, we implement self-attention with ``PyTorch``, which is commonly used when building Transformers.\n",
        "\n",
        "Feel free to change the dimensions of each variable and see how the output dimension will change accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qng07v8xdaPj",
        "outputId": "32652e45-58f4-45b1-e824-446ada27c2df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x.shape:torch.Size([32, 20, 128]) \n",
            " Q.shape:torch.Size([32, 20, 64]) \n",
            " K.shape:torch.Size([32, 20, 64]) \n",
            " V.shape:torch.Size([32, 20, 32])\n",
            "attention matrix:  torch.Size([32, 20, 20])\n",
            "attention outpus:  torch.Size([32, 20, 32])\n",
            "tensor([[[-0.0408,  0.0632, -0.1087,  ..., -0.2198, -0.0873,  0.0345],\n",
            "         [-0.0587,  0.1012, -0.1041,  ..., -0.2198, -0.1100,  0.0832],\n",
            "         [-0.0424,  0.1211, -0.0849,  ..., -0.2848, -0.1433,  0.0299],\n",
            "         ...,\n",
            "         [-0.0569,  0.0842, -0.1462,  ..., -0.2108, -0.0740,  0.0010],\n",
            "         [-0.0562,  0.0348, -0.1415,  ..., -0.2358, -0.0356,  0.0340],\n",
            "         [-0.0695,  0.0524, -0.0828,  ..., -0.2781, -0.0172,  0.1039]],\n",
            "\n",
            "        [[ 0.1277, -0.0648,  0.1376,  ..., -0.0085,  0.2023,  0.1575],\n",
            "         [ 0.2143, -0.1453,  0.0011,  ...,  0.0217,  0.1621,  0.1580],\n",
            "         [ 0.2936, -0.1005,  0.0184,  ...,  0.0715,  0.1990,  0.1594],\n",
            "         ...,\n",
            "         [ 0.2394, -0.1046,  0.0029,  ...,  0.0250,  0.2097,  0.2439],\n",
            "         [ 0.1455, -0.0767,  0.1113,  ..., -0.0687,  0.2965,  0.1250],\n",
            "         [ 0.2971, -0.1357,  0.0488,  ...,  0.0897,  0.0915,  0.1792]],\n",
            "\n",
            "        [[-0.0239, -0.0692,  0.1466,  ..., -0.3198, -0.0713, -0.0848],\n",
            "         [-0.0305, -0.0421,  0.1529,  ..., -0.3280, -0.0696, -0.0879],\n",
            "         [ 0.0362, -0.1136,  0.0767,  ..., -0.3396, -0.0155, -0.0490],\n",
            "         ...,\n",
            "         [-0.0364, -0.0362,  0.1086,  ..., -0.3016, -0.0510, -0.1609],\n",
            "         [-0.0494, -0.0869,  0.1308,  ..., -0.3352,  0.0230, -0.0957],\n",
            "         [ 0.0319, -0.0867,  0.1136,  ..., -0.3531, -0.0755, -0.1292]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0128,  0.0366, -0.0512,  ...,  0.0373, -0.0342,  0.0386],\n",
            "         [-0.0112,  0.2080, -0.0666,  ..., -0.0214,  0.0125,  0.0402],\n",
            "         [ 0.0353,  0.1073, -0.0252,  ...,  0.0633, -0.0773,  0.0596],\n",
            "         ...,\n",
            "         [-0.0079,  0.2121, -0.0314,  ..., -0.0208,  0.0427,  0.0063],\n",
            "         [ 0.1160,  0.0709, -0.0218,  ..., -0.0193, -0.0686,  0.0483],\n",
            "         [-0.0138,  0.1518, -0.0116,  ...,  0.0146, -0.0554, -0.0213]],\n",
            "\n",
            "        [[-0.0764, -0.0404,  0.1953,  ..., -0.0095,  0.0611, -0.3056],\n",
            "         [-0.0374, -0.0657,  0.1182,  ..., -0.0069,  0.0441, -0.2815],\n",
            "         [-0.0560, -0.1360,  0.0518,  ...,  0.0612,  0.0674, -0.2583],\n",
            "         ...,\n",
            "         [-0.0812, -0.0534,  0.1430,  ...,  0.0318, -0.0057, -0.3637],\n",
            "         [-0.1589, -0.0201,  0.1298,  ...,  0.0248,  0.0775, -0.2631],\n",
            "         [-0.1145, -0.0511,  0.1438,  ...,  0.0464,  0.0325, -0.3342]],\n",
            "\n",
            "        [[ 0.0005,  0.0028,  0.1524,  ...,  0.1307, -0.1183,  0.1794],\n",
            "         [-0.0264,  0.0589,  0.0808,  ...,  0.1367, -0.1222,  0.1343],\n",
            "         [-0.0512, -0.0065,  0.0978,  ...,  0.1701, -0.1040,  0.1848],\n",
            "         ...,\n",
            "         [-0.0461,  0.0864,  0.1596,  ...,  0.0560, -0.0529,  0.1909],\n",
            "         [-0.0116,  0.0657,  0.1848,  ...,  0.1005, -0.0976,  0.2365],\n",
            "         [ 0.0080, -0.0015,  0.1324,  ...,  0.0809, -0.0621,  0.1418]]],\n",
            "       grad_fn=<BmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, dim_input, dim_q, dim_v):\n",
        "        '''\n",
        "        dim_input: the dimension of each sample\n",
        "        dim_q: dimension of Q matrix, should be equal to dim_k\n",
        "        dim_v: dimension of V matrix, also the the dimension of the attention output\n",
        "        '''\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        self.dim_input = dim_input\n",
        "        self.dim_q = dim_q\n",
        "        self.dim_k = dim_q\n",
        "        self.dim_v = dim_v\n",
        "\n",
        "        # Define the linear projection\n",
        "        self.linear_q = nn.Linear(self.dim_input, self.dim_q, bias=False)\n",
        "        self.linear_k = nn.Linear(self.dim_input, self.dim_k, bias=False)\n",
        "        self.linear_v = nn.Linear(self.dim_input, self.dim_v, bias=False)\n",
        "        self._norm_fact = 1 / math.sqrt(self.dim_k)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, n, dim_q = x.shape\n",
        "\n",
        "        q = self.linear_q(x) # (batchsize, seq_len, dim_q)\n",
        "        k = self.linear_k(x) # (batchsize, seq_len, dim_k)\n",
        "        v = self.linear_v(x) # (batchsize, seq_len, dim_v)\n",
        "        print(f'x.shape:{x.shape} \\n Q.shape:{q.shape} \\n K.shape:{k.shape} \\n V.shape:{v.shape}')\n",
        "\n",
        "        dist = torch.bmm(q, k.transpose(1,2)) * self._norm_fact\n",
        "        dist = torch.softmax(dist, dim=-1)\n",
        "        print('attention matrix: ', dist.shape)\n",
        "\n",
        "        outputs = torch.bmm(dist, v)\n",
        "        print('attention outpus: ', outputs.shape)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    batch_size = 32 # number of samples in a batch\n",
        "    dim_input = 128 # dimension of each item in the sample sequence\n",
        "    seq_len = 20 # sequence length for each sample\n",
        "    x = torch.randn(batch_size, seq_len, dim_input)\n",
        "    self_attention = SelfAttention(dim_input, dim_q = 64, dim_v = 32)\n",
        "\n",
        "    attention = self_attention(x)\n",
        "\n",
        "    print(attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZaAFL8MS2Ng"
      },
      "source": [
        "# Transformers\n",
        "In this section, we implement a 6-layer Vision Transformer (ViT) and trained it on the MNIST dataset.\n",
        "We consider the classification tasks.\n",
        "First, we load the MNIST dataset as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ-eIaeZjWjL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets, utils\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "def get_mnist_loader(batch_size=100, shuffle=True):\n",
        "    \"\"\"\n",
        "\n",
        "    :return: train_loader, test_loader\n",
        "    \"\"\"\n",
        "    train_dataset = MNIST(root='../data',\n",
        "                          train=True,\n",
        "                          transform=torchvision.transforms.ToTensor(),\n",
        "                          download=True)\n",
        "    test_dataset = MNIST(root='../data',\n",
        "                         train=False,\n",
        "                         transform=torchvision.transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=shuffle)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C06IoPIjePg",
        "outputId": "2c6dba39-7cc7-4c36-c55e-a7a88b138ad6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m578.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ],
      "source": [
        "# This package is needed to build the transformer\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx9eZrMpmA2z"
      },
      "source": [
        "## Build ViT from scratch\n",
        "Recall that each Transformer block include 2 modules: the self-attention module, the feedforward module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr6d7IWfjpxY"
      },
      "outputs": [],
      "source": [
        "from einops import rearrange\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=8):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = dim ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x)\n",
        "        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv=3, h=h)\n",
        "\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\n",
        "            dots.masked_fill_(~mask, float('-inf'))\n",
        "            del mask\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "        return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Residual(PreNorm(dim, Attention(dim, heads = heads))),\n",
        "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim)))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, mask=mask)\n",
        "            x = ff(x)\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = channels * patch_size ** 2\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.transformer = Transformer(dim, depth, heads, mlp_dim)\n",
        "\n",
        "        self.to_cls_token = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img, mask=None):\n",
        "        p = self.patch_size\n",
        "\n",
        "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
        "        x = self.patch_to_embedding(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding\n",
        "        x = self.transformer(x, mask)\n",
        "\n",
        "        x = self.to_cls_token(x[:, 0])\n",
        "        return self.mlp_head(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTawNC64mhBO"
      },
      "source": [
        "## Training and test function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKJ4tjCjjycH"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train_epoch(model, optimizer, data_loader, loss_history):\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    model.train()\n",
        "\n",
        "    for i, (data, target) in enumerate(data_loader):\n",
        "        data = data.cuda()\n",
        "        target = target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = F.log_softmax(model(data), dim=1)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
        "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\n",
        "                  '{:6.4f}'.format(loss.item()))\n",
        "            loss_history.append(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vph2CrNxj6ZZ"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, loss_history):\n",
        "    model.eval()\n",
        "\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    correct_samples = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    # We do not need to remeber the gradients when testing\n",
        "    # This will help reduce memory\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data = data.cuda()\n",
        "            target = target.cuda()\n",
        "            output = F.log_softmax(model(data), dim=1)\n",
        "            loss = F.nll_loss(output, target, reduction='sum')\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct_samples += pred.eq(target).sum()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    loss_history.append(avg_loss)\n",
        "    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
        "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
        "          '{:5}'.format(total_samples) + ' (' +\n",
        "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')\n",
        "    return avg_loss, correct_samples, total_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRYys50km0-E"
      },
      "source": [
        "## Let's start training!\n",
        "Here, you can change the ViT structure by changing the hyper-parametrs inside ``ViT`` function.\n",
        "The default settings are with 6 layers, 8 heads for the multi-head attention mechanism and embedding dimension of 64.\n",
        "You can also increase the number of epochs to obtain better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVLJLLDuj7yQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# You can change the architecture here\n",
        "model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1,\n",
        "            dim=64, depth=6, heads=8, mlp_dim=128)\n",
        "model = model.cuda()\n",
        "# We also print the network architecture\n",
        "model\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train_loss_history, test_loss_history = [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vlt3tk-MkDB9",
        "outputId": "432352f8-e00f-46e0-ba7c-6bd1b1115660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 105261094.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 28705140.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 31944661.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 22074772.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3925\n",
            "[12800/60000 ( 21%)]  Loss: 1.0473\n",
            "[25600/60000 ( 43%)]  Loss: 0.4524\n",
            "[38400/60000 ( 64%)]  Loss: 0.2258\n",
            "[51200/60000 ( 85%)]  Loss: 0.2075\n",
            "\n",
            "Average test loss: 0.2030  Accuracy: 9379/10000 (93.79%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.2539\n",
            "[12800/60000 ( 21%)]  Loss: 0.2008\n",
            "[25600/60000 ( 43%)]  Loss: 0.1333\n",
            "[38400/60000 ( 64%)]  Loss: 0.2377\n",
            "[51200/60000 ( 85%)]  Loss: 0.1250\n",
            "\n",
            "Average test loss: 0.1176  Accuracy: 9629/10000 (96.29%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.0560\n",
            "[12800/60000 ( 21%)]  Loss: 0.1644\n",
            "[25600/60000 ( 43%)]  Loss: 0.1672\n",
            "[38400/60000 ( 64%)]  Loss: 0.2097\n",
            "[51200/60000 ( 85%)]  Loss: 0.1582\n",
            "\n",
            "Average test loss: 0.1162  Accuracy: 9635/10000 (96.35%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.0897\n",
            "[12800/60000 ( 21%)]  Loss: 0.0964\n",
            "[25600/60000 ( 43%)]  Loss: 0.1208\n",
            "[38400/60000 ( 64%)]  Loss: 0.1058\n",
            "[51200/60000 ( 85%)]  Loss: 0.0187\n",
            "\n",
            "Average test loss: 0.0800  Accuracy: 9738/10000 (97.38%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0149\n",
            "[12800/60000 ( 21%)]  Loss: 0.0068\n",
            "[25600/60000 ( 43%)]  Loss: 0.0673\n",
            "[38400/60000 ( 64%)]  Loss: 0.0580\n",
            "[51200/60000 ( 85%)]  Loss: 0.0709\n",
            "\n",
            "Average test loss: 0.0807  Accuracy: 9757/10000 (97.57%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0354\n",
            "[12800/60000 ( 21%)]  Loss: 0.0362\n",
            "[25600/60000 ( 43%)]  Loss: 0.0833\n",
            "[38400/60000 ( 64%)]  Loss: 0.0947\n",
            "[51200/60000 ( 85%)]  Loss: 0.1361\n",
            "\n",
            "Average test loss: 0.0706  Accuracy: 9773/10000 (97.73%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0308\n",
            "[12800/60000 ( 21%)]  Loss: 0.0134\n",
            "[25600/60000 ( 43%)]  Loss: 0.0121\n",
            "[38400/60000 ( 64%)]  Loss: 0.0065\n",
            "[51200/60000 ( 85%)]  Loss: 0.0470\n",
            "\n",
            "Average test loss: 0.0649  Accuracy: 9789/10000 (97.89%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0312\n",
            "[12800/60000 ( 21%)]  Loss: 0.0274\n",
            "[25600/60000 ( 43%)]  Loss: 0.0406\n",
            "[38400/60000 ( 64%)]  Loss: 0.0285\n",
            "[51200/60000 ( 85%)]  Loss: 0.0344\n",
            "\n",
            "Average test loss: 0.0735  Accuracy: 9792/10000 (97.92%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0151\n",
            "[12800/60000 ( 21%)]  Loss: 0.0096\n",
            "[25600/60000 ( 43%)]  Loss: 0.0364\n",
            "[38400/60000 ( 64%)]  Loss: 0.0470\n",
            "[51200/60000 ( 85%)]  Loss: 0.0209\n",
            "\n",
            "Average test loss: 0.0632  Accuracy: 9820/10000 (98.20%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0211\n",
            "[12800/60000 ( 21%)]  Loss: 0.0103\n",
            "[25600/60000 ( 43%)]  Loss: 0.0200\n",
            "[38400/60000 ( 64%)]  Loss: 0.0582\n",
            "[51200/60000 ( 85%)]  Loss: 0.0075\n",
            "\n",
            "Average test loss: 0.0578  Accuracy: 9829/10000 (98.29%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0230\n",
            "[12800/60000 ( 21%)]  Loss: 0.0031\n",
            "[25600/60000 ( 43%)]  Loss: 0.0132\n",
            "[38400/60000 ( 64%)]  Loss: 0.0136\n",
            "[51200/60000 ( 85%)]  Loss: 0.0129\n",
            "\n",
            "Average test loss: 0.0744  Accuracy: 9798/10000 (97.98%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0425\n",
            "[12800/60000 ( 21%)]  Loss: 0.0237\n",
            "[25600/60000 ( 43%)]  Loss: 0.0042\n",
            "[38400/60000 ( 64%)]  Loss: 0.0197\n",
            "[51200/60000 ( 85%)]  Loss: 0.0355\n",
            "\n",
            "Average test loss: 0.0728  Accuracy: 9816/10000 (98.16%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0326\n",
            "[12800/60000 ( 21%)]  Loss: 0.0248\n",
            "[25600/60000 ( 43%)]  Loss: 0.0035\n",
            "[38400/60000 ( 64%)]  Loss: 0.0078\n",
            "[51200/60000 ( 85%)]  Loss: 0.0263\n",
            "\n",
            "Average test loss: 0.0733  Accuracy: 9802/10000 (98.02%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0033\n",
            "[12800/60000 ( 21%)]  Loss: 0.0014\n",
            "[25600/60000 ( 43%)]  Loss: 0.0027\n",
            "[38400/60000 ( 64%)]  Loss: 0.0068\n",
            "[51200/60000 ( 85%)]  Loss: 0.0023\n",
            "\n",
            "Average test loss: 0.0869  Accuracy: 9787/10000 (97.87%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0007\n",
            "[12800/60000 ( 21%)]  Loss: 0.0031\n",
            "[25600/60000 ( 43%)]  Loss: 0.0278\n",
            "[38400/60000 ( 64%)]  Loss: 0.0126\n",
            "[51200/60000 ( 85%)]  Loss: 0.0289\n",
            "\n",
            "Average test loss: 0.0632  Accuracy: 9836/10000 (98.36%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0190\n",
            "[12800/60000 ( 21%)]  Loss: 0.0170\n",
            "[25600/60000 ( 43%)]  Loss: 0.0063\n",
            "[38400/60000 ( 64%)]  Loss: 0.0273\n",
            "[51200/60000 ( 85%)]  Loss: 0.0328\n",
            "\n",
            "Average test loss: 0.0766  Accuracy: 9828/10000 (98.28%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0189\n",
            "[12800/60000 ( 21%)]  Loss: 0.0035\n",
            "[25600/60000 ( 43%)]  Loss: 0.0247\n",
            "[38400/60000 ( 64%)]  Loss: 0.0046\n",
            "[51200/60000 ( 85%)]  Loss: 0.0004\n",
            "\n",
            "Average test loss: 0.0805  Accuracy: 9804/10000 (98.04%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0025\n",
            "[12800/60000 ( 21%)]  Loss: 0.0011\n",
            "[25600/60000 ( 43%)]  Loss: 0.0061\n",
            "[38400/60000 ( 64%)]  Loss: 0.0012\n",
            "[51200/60000 ( 85%)]  Loss: 0.0295\n",
            "\n",
            "Average test loss: 0.0785  Accuracy: 9840/10000 (98.40%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0033\n",
            "[12800/60000 ( 21%)]  Loss: 0.0010\n",
            "[25600/60000 ( 43%)]  Loss: 0.0070\n",
            "[38400/60000 ( 64%)]  Loss: 0.0021\n",
            "[51200/60000 ( 85%)]  Loss: 0.0197\n",
            "\n",
            "Average test loss: 0.0624  Accuracy: 9857/10000 (98.57%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0006\n",
            "[12800/60000 ( 21%)]  Loss: 0.0009\n",
            "[25600/60000 ( 43%)]  Loss: 0.0118\n",
            "[38400/60000 ( 64%)]  Loss: 0.0001\n",
            "[51200/60000 ( 85%)]  Loss: 0.0001\n",
            "\n",
            "Average test loss: 0.0687  Accuracy: 9846/10000 (98.46%)\n",
            "\n",
            "Execution time: 304.07 seconds\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "train_loader, test_loader = get_mnist_loader(batch_size=128, shuffle=True)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
        "    train_epoch(model, optimizer, train_loader, train_loss_history)\n",
        "    evaluate(model, test_loader, test_loss_history)\n",
        "    scheduler.step()\n",
        "\n",
        "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XicoRf8_nUTK"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_and_evaluate(config):\n",
        "    model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1,\n",
        "                dim=config['dim'], depth=config['depth'], heads=config['heads'], mlp_dim=config['mlp_dim'])\n",
        "    model = model.cuda()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "    train_loss_history, test_loss_history = [], []\n",
        "    N_EPOCHS = 20\n",
        "    train_loader, test_loader = get_mnist_loader(batch_size=128, shuffle=True)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(1, N_EPOCHS + 1):\n",
        "        print('Config:', config, 'Epoch:', epoch, 'LR:', scheduler.get_last_lr())\n",
        "        train_epoch(model, optimizer, train_loader, train_loss_history)\n",
        "        avg_loss, correct_samples, total_samples = evaluate(model, test_loader, test_loss_history)\n",
        "        scheduler.step()\n",
        "\n",
        "    execution_time = time.time() - start_time\n",
        "    last_epoch_accuracy = 100.0 * correct_samples / total_samples\n",
        "\n",
        "    return execution_time, last_epoch_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zjBlXhMHv1Nh",
        "outputId": "b29ec720-6d53-4a01-ed7f-760c5eaf76f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128} Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3193\n",
            "[12800/60000 ( 21%)]  Loss: 1.0016\n",
            "[25600/60000 ( 43%)]  Loss: 0.4204\n",
            "[38400/60000 ( 64%)]  Loss: 0.4558\n",
            "[51200/60000 ( 85%)]  Loss: 0.2172\n",
            "\n",
            "Average test loss: 0.2009  Accuracy: 9385/10000 (93.85%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128} Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.2695\n",
            "[12800/60000 ( 21%)]  Loss: 0.1358\n",
            "[25600/60000 ( 43%)]  Loss: 0.1361\n",
            "[38400/60000 ( 64%)]  Loss: 0.1621\n",
            "[51200/60000 ( 85%)]  Loss: 0.1401\n",
            "\n",
            "Average test loss: 0.1449  Accuracy: 9521/10000 (95.21%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128} Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.1110\n",
            "[12800/60000 ( 21%)]  Loss: 0.0970\n",
            "[25600/60000 ( 43%)]  Loss: 0.1008\n",
            "[38400/60000 ( 64%)]  Loss: 0.0350\n",
            "[51200/60000 ( 85%)]  Loss: 0.1014\n",
            "\n",
            "Average test loss: 0.1304  Accuracy: 9580/10000 (95.80%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128} Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.1620\n",
            "[12800/60000 ( 21%)]  Loss: 0.1056\n",
            "[25600/60000 ( 43%)]  Loss: 0.0733\n",
            "[38400/60000 ( 64%)]  Loss: 0.0432\n",
            "[51200/60000 ( 85%)]  Loss: 0.1474\n",
            "\n",
            "Average test loss: 0.0910  Accuracy: 9708/10000 (97.08%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128} Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.1083\n",
            "[12800/60000 ( 21%)]  Loss: 0.0295\n",
            "[25600/60000 ( 43%)]  Loss: 0.0901\n",
            "[38400/60000 ( 64%)]  Loss: 0.0706\n",
            "[51200/60000 ( 85%)]  Loss: 0.0477\n",
            "\n",
            "Average test loss: 0.0829  Accuracy: 9743/10000 (97.43%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128} Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0411\n",
            "[12800/60000 ( 21%)]  Loss: 0.0597\n",
            "[25600/60000 ( 43%)]  Loss: 0.0873\n",
            "[38400/60000 ( 64%)]  Loss: 0.0508\n",
            "[51200/60000 ( 85%)]  Loss: 0.0295\n",
            "\n",
            "Average test loss: 0.0716  Accuracy: 9781/10000 (97.81%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128} Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0139\n",
            "[12800/60000 ( 21%)]  Loss: 0.0118\n",
            "[25600/60000 ( 43%)]  Loss: 0.0588\n",
            "[38400/60000 ( 64%)]  Loss: 0.0510\n",
            "[51200/60000 ( 85%)]  Loss: 0.0989\n",
            "\n",
            "Average test loss: 0.0686  Accuracy: 9782/10000 (97.82%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128} Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0308\n",
            "[12800/60000 ( 21%)]  Loss: 0.0357\n",
            "[25600/60000 ( 43%)]  Loss: 0.0385\n",
            "[38400/60000 ( 64%)]  Loss: 0.0352\n",
            "[51200/60000 ( 85%)]  Loss: 0.0174\n",
            "\n",
            "Average test loss: 0.0821  Accuracy: 9744/10000 (97.44%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128} Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0285\n",
            "[12800/60000 ( 21%)]  Loss: 0.0313\n",
            "[25600/60000 ( 43%)]  Loss: 0.0394\n",
            "[38400/60000 ( 64%)]  Loss: 0.0166\n",
            "[51200/60000 ( 85%)]  Loss: 0.0417\n",
            "\n",
            "Average test loss: 0.0681  Accuracy: 9807/10000 (98.07%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128} Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.1072\n",
            "[12800/60000 ( 21%)]  Loss: 0.0271\n",
            "[25600/60000 ( 43%)]  Loss: 0.0379\n",
            "[38400/60000 ( 64%)]  Loss: 0.0207\n",
            "[51200/60000 ( 85%)]  Loss: 0.0168\n",
            "\n",
            "Average test loss: 0.0662  Accuracy: 9815/10000 (98.15%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128} Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0221\n",
            "[12800/60000 ( 21%)]  Loss: 0.0211\n",
            "[25600/60000 ( 43%)]  Loss: 0.0371\n",
            "[38400/60000 ( 64%)]  Loss: 0.0322\n",
            "[51200/60000 ( 85%)]  Loss: 0.0218\n",
            "\n",
            "Average test loss: 0.0599  Accuracy: 9821/10000 (98.21%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128} Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0199\n",
            "[12800/60000 ( 21%)]  Loss: 0.0007\n",
            "[25600/60000 ( 43%)]  Loss: 0.0405\n",
            "[38400/60000 ( 64%)]  Loss: 0.0097\n",
            "[51200/60000 ( 85%)]  Loss: 0.0311\n",
            "\n",
            "Average test loss: 0.0631  Accuracy: 9820/10000 (98.20%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128} Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0134\n",
            "[12800/60000 ( 21%)]  Loss: 0.0044\n",
            "[25600/60000 ( 43%)]  Loss: 0.0079\n",
            "[38400/60000 ( 64%)]  Loss: 0.0520\n",
            "[51200/60000 ( 85%)]  Loss: 0.0070\n",
            "\n",
            "Average test loss: 0.0650  Accuracy: 9811/10000 (98.11%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128} Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0031\n",
            "[12800/60000 ( 21%)]  Loss: 0.0009\n",
            "[25600/60000 ( 43%)]  Loss: 0.0006\n",
            "[38400/60000 ( 64%)]  Loss: 0.0031\n",
            "[51200/60000 ( 85%)]  Loss: 0.0043\n",
            "\n",
            "Average test loss: 0.0674  Accuracy: 9810/10000 (98.10%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128} Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0354\n",
            "[12800/60000 ( 21%)]  Loss: 0.0006\n",
            "[25600/60000 ( 43%)]  Loss: 0.0185\n",
            "[38400/60000 ( 64%)]  Loss: 0.0052\n",
            "[51200/60000 ( 85%)]  Loss: 0.0043\n",
            "\n",
            "Average test loss: 0.0659  Accuracy: 9834/10000 (98.34%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128} Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0124\n",
            "[12800/60000 ( 21%)]  Loss: 0.0092\n",
            "[25600/60000 ( 43%)]  Loss: 0.0078\n",
            "[38400/60000 ( 64%)]  Loss: 0.0189\n",
            "[51200/60000 ( 85%)]  Loss: 0.0273\n",
            "\n",
            "Average test loss: 0.0655  Accuracy: 9828/10000 (98.28%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128} Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0015\n",
            "[12800/60000 ( 21%)]  Loss: 0.0005\n",
            "[25600/60000 ( 43%)]  Loss: 0.0354\n",
            "[38400/60000 ( 64%)]  Loss: 0.0038\n",
            "[51200/60000 ( 85%)]  Loss: 0.0008\n",
            "\n",
            "Average test loss: 0.0677  Accuracy: 9824/10000 (98.24%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128} Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0052\n",
            "[12800/60000 ( 21%)]  Loss: 0.0079\n",
            "[25600/60000 ( 43%)]  Loss: 0.0012\n",
            "[38400/60000 ( 64%)]  Loss: 0.0145\n",
            "[51200/60000 ( 85%)]  Loss: 0.0025\n",
            "\n",
            "Average test loss: 0.0724  Accuracy: 9825/10000 (98.25%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128} Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0033\n",
            "[12800/60000 ( 21%)]  Loss: 0.0113\n",
            "[25600/60000 ( 43%)]  Loss: 0.0041\n",
            "[38400/60000 ( 64%)]  Loss: 0.0016\n",
            "[51200/60000 ( 85%)]  Loss: 0.0046\n",
            "\n",
            "Average test loss: 0.0753  Accuracy: 9811/10000 (98.11%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128} Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0050\n",
            "[12800/60000 ( 21%)]  Loss: 0.0019\n",
            "[25600/60000 ( 43%)]  Loss: 0.0350\n",
            "[38400/60000 ( 64%)]  Loss: 0.0020\n",
            "[51200/60000 ( 85%)]  Loss: 0.0006\n",
            "\n",
            "Average test loss: 0.0710  Accuracy: 9843/10000 (98.43%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256} Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3870\n",
            "[12800/60000 ( 21%)]  Loss: 0.6849\n",
            "[25600/60000 ( 43%)]  Loss: 0.2838\n",
            "[38400/60000 ( 64%)]  Loss: 0.2903\n",
            "[51200/60000 ( 85%)]  Loss: 0.2111\n",
            "\n",
            "Average test loss: 0.1356  Accuracy: 9568/10000 (95.68%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256} Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.0551\n",
            "[12800/60000 ( 21%)]  Loss: 0.0537\n",
            "[25600/60000 ( 43%)]  Loss: 0.1576\n",
            "[38400/60000 ( 64%)]  Loss: 0.0511\n",
            "[51200/60000 ( 85%)]  Loss: 0.1206\n",
            "\n",
            "Average test loss: 0.0964  Accuracy: 9681/10000 (96.81%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256} Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.0496\n",
            "[12800/60000 ( 21%)]  Loss: 0.0816\n",
            "[25600/60000 ( 43%)]  Loss: 0.1802\n",
            "[38400/60000 ( 64%)]  Loss: 0.0822\n",
            "[51200/60000 ( 85%)]  Loss: 0.1284\n",
            "\n",
            "Average test loss: 0.0817  Accuracy: 9749/10000 (97.49%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256} Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.0309\n",
            "[12800/60000 ( 21%)]  Loss: 0.1175\n",
            "[25600/60000 ( 43%)]  Loss: 0.0691\n",
            "[38400/60000 ( 64%)]  Loss: 0.0141\n",
            "[51200/60000 ( 85%)]  Loss: 0.0851\n",
            "\n",
            "Average test loss: 0.0692  Accuracy: 9754/10000 (97.54%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256} Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0081\n",
            "[12800/60000 ( 21%)]  Loss: 0.0803\n",
            "[25600/60000 ( 43%)]  Loss: 0.0725\n",
            "[38400/60000 ( 64%)]  Loss: 0.0335\n",
            "[51200/60000 ( 85%)]  Loss: 0.0527\n",
            "\n",
            "Average test loss: 0.0755  Accuracy: 9756/10000 (97.56%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256} Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0289\n",
            "[12800/60000 ( 21%)]  Loss: 0.0618\n",
            "[25600/60000 ( 43%)]  Loss: 0.0135\n",
            "[38400/60000 ( 64%)]  Loss: 0.0307\n",
            "[51200/60000 ( 85%)]  Loss: 0.0798\n",
            "\n",
            "Average test loss: 0.0721  Accuracy: 9794/10000 (97.94%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256} Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0509\n",
            "[12800/60000 ( 21%)]  Loss: 0.0421\n",
            "[25600/60000 ( 43%)]  Loss: 0.0080\n",
            "[38400/60000 ( 64%)]  Loss: 0.0213\n",
            "[51200/60000 ( 85%)]  Loss: 0.0976\n",
            "\n",
            "Average test loss: 0.0808  Accuracy: 9751/10000 (97.51%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256} Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0983\n",
            "[12800/60000 ( 21%)]  Loss: 0.0232\n",
            "[25600/60000 ( 43%)]  Loss: 0.0175\n",
            "[38400/60000 ( 64%)]  Loss: 0.0225\n",
            "[51200/60000 ( 85%)]  Loss: 0.0786\n",
            "\n",
            "Average test loss: 0.0614  Accuracy: 9821/10000 (98.21%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256} Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0360\n",
            "[12800/60000 ( 21%)]  Loss: 0.0451\n",
            "[25600/60000 ( 43%)]  Loss: 0.0752\n",
            "[38400/60000 ( 64%)]  Loss: 0.0161\n",
            "[51200/60000 ( 85%)]  Loss: 0.0147\n",
            "\n",
            "Average test loss: 0.0740  Accuracy: 9783/10000 (97.83%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256} Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0486\n",
            "[12800/60000 ( 21%)]  Loss: 0.0017\n",
            "[25600/60000 ( 43%)]  Loss: 0.0207\n",
            "[38400/60000 ( 64%)]  Loss: 0.0319\n",
            "[51200/60000 ( 85%)]  Loss: 0.0069\n",
            "\n",
            "Average test loss: 0.0565  Accuracy: 9835/10000 (98.35%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256} Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0234\n",
            "[12800/60000 ( 21%)]  Loss: 0.0313\n",
            "[25600/60000 ( 43%)]  Loss: 0.0122\n",
            "[38400/60000 ( 64%)]  Loss: 0.1168\n",
            "[51200/60000 ( 85%)]  Loss: 0.0124\n",
            "\n",
            "Average test loss: 0.0597  Accuracy: 9842/10000 (98.42%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256} Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0085\n",
            "[12800/60000 ( 21%)]  Loss: 0.0120\n",
            "[25600/60000 ( 43%)]  Loss: 0.0219\n",
            "[38400/60000 ( 64%)]  Loss: 0.0055\n",
            "[51200/60000 ( 85%)]  Loss: 0.0028\n",
            "\n",
            "Average test loss: 0.0539  Accuracy: 9851/10000 (98.51%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256} Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0009\n",
            "[12800/60000 ( 21%)]  Loss: 0.0100\n",
            "[25600/60000 ( 43%)]  Loss: 0.0087\n",
            "[38400/60000 ( 64%)]  Loss: 0.0031\n",
            "[51200/60000 ( 85%)]  Loss: 0.0380\n",
            "\n",
            "Average test loss: 0.0531  Accuracy: 9855/10000 (98.55%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256} Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0156\n",
            "[12800/60000 ( 21%)]  Loss: 0.0012\n",
            "[25600/60000 ( 43%)]  Loss: 0.0156\n",
            "[38400/60000 ( 64%)]  Loss: 0.0113\n",
            "[51200/60000 ( 85%)]  Loss: 0.0359\n",
            "\n",
            "Average test loss: 0.0624  Accuracy: 9831/10000 (98.31%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256} Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0532\n",
            "[12800/60000 ( 21%)]  Loss: 0.0008\n",
            "[25600/60000 ( 43%)]  Loss: 0.0234\n",
            "[38400/60000 ( 64%)]  Loss: 0.0002\n",
            "[51200/60000 ( 85%)]  Loss: 0.0201\n",
            "\n",
            "Average test loss: 0.0656  Accuracy: 9841/10000 (98.41%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256} Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0131\n",
            "[12800/60000 ( 21%)]  Loss: 0.0033\n",
            "[25600/60000 ( 43%)]  Loss: 0.0358\n",
            "[38400/60000 ( 64%)]  Loss: 0.0014\n",
            "[51200/60000 ( 85%)]  Loss: 0.0002\n",
            "\n",
            "Average test loss: 0.0646  Accuracy: 9857/10000 (98.57%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256} Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0090\n",
            "[12800/60000 ( 21%)]  Loss: 0.0008\n",
            "[25600/60000 ( 43%)]  Loss: 0.0001\n",
            "[38400/60000 ( 64%)]  Loss: 0.0067\n",
            "[51200/60000 ( 85%)]  Loss: 0.0035\n",
            "\n",
            "Average test loss: 0.0588  Accuracy: 9864/10000 (98.64%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256} Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0002\n",
            "[12800/60000 ( 21%)]  Loss: 0.0068\n",
            "[25600/60000 ( 43%)]  Loss: 0.0002\n",
            "[38400/60000 ( 64%)]  Loss: 0.0003\n",
            "[51200/60000 ( 85%)]  Loss: 0.0024\n",
            "\n",
            "Average test loss: 0.0803  Accuracy: 9839/10000 (98.39%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256} Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0062\n",
            "[12800/60000 ( 21%)]  Loss: 0.0013\n",
            "[25600/60000 ( 43%)]  Loss: 0.0329\n",
            "[38400/60000 ( 64%)]  Loss: 0.0004\n",
            "[51200/60000 ( 85%)]  Loss: 0.0031\n",
            "\n",
            "Average test loss: 0.0567  Accuracy: 9873/10000 (98.73%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256} Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0005\n",
            "[12800/60000 ( 21%)]  Loss: 0.0010\n",
            "[25600/60000 ( 43%)]  Loss: 0.0029\n",
            "[38400/60000 ( 64%)]  Loss: 0.0006\n",
            "[51200/60000 ( 85%)]  Loss: 0.0241\n",
            "\n",
            "Average test loss: 0.0636  Accuracy: 9867/10000 (98.67%)\n",
            "\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64} Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3241\n",
            "[12800/60000 ( 21%)]  Loss: 1.4346\n",
            "[25600/60000 ( 43%)]  Loss: 0.7299\n",
            "[38400/60000 ( 64%)]  Loss: 0.5120\n",
            "[51200/60000 ( 85%)]  Loss: 0.2279\n",
            "\n",
            "Average test loss: 0.2629  Accuracy: 9195/10000 (91.95%)\n",
            "\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64} Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.3326\n",
            "[12800/60000 ( 21%)]  Loss: 0.2407\n",
            "[25600/60000 ( 43%)]  Loss: 0.1801\n",
            "[38400/60000 ( 64%)]  Loss: 0.2369\n",
            "[51200/60000 ( 85%)]  Loss: 0.1602\n",
            "\n",
            "Average test loss: 0.1609  Accuracy: 9498/10000 (94.98%)\n",
            "\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64} Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.0983\n",
            "[12800/60000 ( 21%)]  Loss: 0.1976\n",
            "[25600/60000 ( 43%)]  Loss: 0.1382\n",
            "[38400/60000 ( 64%)]  Loss: 0.0695\n",
            "[51200/60000 ( 85%)]  Loss: 0.1583\n",
            "\n",
            "Average test loss: 0.1319  Accuracy: 9595/10000 (95.95%)\n",
            "\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64} Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.1595\n",
            "[12800/60000 ( 21%)]  Loss: 0.2037\n",
            "[25600/60000 ( 43%)]  Loss: 0.0771\n",
            "[38400/60000 ( 64%)]  Loss: 0.0659\n",
            "[51200/60000 ( 85%)]  Loss: 0.1562\n",
            "\n",
            "Average test loss: 0.1289  Accuracy: 9584/10000 (95.84%)\n",
            "\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64} Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0741\n",
            "[12800/60000 ( 21%)]  Loss: 0.0629\n",
            "[25600/60000 ( 43%)]  Loss: 0.1190\n",
            "[38400/60000 ( 64%)]  Loss: 0.0903\n",
            "[51200/60000 ( 85%)]  Loss: 0.1859\n",
            "\n",
            "Average test loss: 0.1004  Accuracy: 9699/10000 (96.99%)\n",
            "\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64} Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0533\n",
            "[12800/60000 ( 21%)]  Loss: 0.1831\n",
            "[25600/60000 ( 43%)]  Loss: 0.0986\n",
            "[38400/60000 ( 64%)]  Loss: 0.0447\n",
            "[51200/60000 ( 85%)]  Loss: 0.0327\n",
            "\n",
            "Average test loss: 0.0909  Accuracy: 9716/10000 (97.16%)\n",
            "\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64} Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0689\n",
            "[12800/60000 ( 21%)]  Loss: 0.1583\n",
            "[25600/60000 ( 43%)]  Loss: 0.0967\n",
            "[38400/60000 ( 64%)]  Loss: 0.0381\n",
            "[51200/60000 ( 85%)]  Loss: 0.0941\n",
            "\n",
            "Average test loss: 0.0934  Accuracy: 9706/10000 (97.06%)\n",
            "\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64} Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0455\n",
            "[12800/60000 ( 21%)]  Loss: 0.0204\n",
            "[25600/60000 ( 43%)]  Loss: 0.0365\n",
            "[38400/60000 ( 64%)]  Loss: 0.0624\n",
            "[51200/60000 ( 85%)]  Loss: 0.0229\n",
            "\n",
            "Average test loss: 0.0749  Accuracy: 9756/10000 (97.56%)\n",
            "\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64} Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0559\n",
            "[12800/60000 ( 21%)]  Loss: 0.0652\n",
            "[25600/60000 ( 43%)]  Loss: 0.0870\n",
            "[38400/60000 ( 64%)]  Loss: 0.1130\n",
            "[51200/60000 ( 85%)]  Loss: 0.0287\n",
            "\n",
            "Average test loss: 0.0856  Accuracy: 9747/10000 (97.47%)\n",
            "\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64} Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0069\n",
            "[12800/60000 ( 21%)]  Loss: 0.0744\n",
            "[25600/60000 ( 43%)]  Loss: 0.1539\n",
            "[38400/60000 ( 64%)]  Loss: 0.0100\n",
            "[51200/60000 ( 85%)]  Loss: 0.0818\n",
            "\n",
            "Average test loss: 0.0723  Accuracy: 9787/10000 (97.87%)\n",
            "\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64} Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0147\n",
            "[12800/60000 ( 21%)]  Loss: 0.0801\n",
            "[25600/60000 ( 43%)]  Loss: 0.0750\n",
            "[38400/60000 ( 64%)]  Loss: 0.0430\n",
            "[51200/60000 ( 85%)]  Loss: 0.1190\n",
            "\n",
            "Average test loss: 0.0906  Accuracy: 9715/10000 (97.15%)\n",
            "\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64} Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0150\n",
            "[12800/60000 ( 21%)]  Loss: 0.0322\n",
            "[25600/60000 ( 43%)]  Loss: 0.0713\n",
            "[38400/60000 ( 64%)]  Loss: 0.1086\n",
            "[51200/60000 ( 85%)]  Loss: 0.0087\n",
            "\n",
            "Average test loss: 0.0727  Accuracy: 9781/10000 (97.81%)\n",
            "\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64} Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0310\n",
            "[12800/60000 ( 21%)]  Loss: 0.1224\n",
            "[25600/60000 ( 43%)]  Loss: 0.0481\n",
            "[38400/60000 ( 64%)]  Loss: 0.0536\n",
            "[51200/60000 ( 85%)]  Loss: 0.0235\n",
            "\n",
            "Average test loss: 0.0865  Accuracy: 9747/10000 (97.47%)\n",
            "\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64} Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0210\n",
            "[12800/60000 ( 21%)]  Loss: 0.0167\n",
            "[25600/60000 ( 43%)]  Loss: 0.0586\n",
            "[38400/60000 ( 64%)]  Loss: 0.0359\n",
            "[51200/60000 ( 85%)]  Loss: 0.0247\n",
            "\n",
            "Average test loss: 0.0709  Accuracy: 9791/10000 (97.91%)\n",
            "\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64} Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0148\n",
            "[12800/60000 ( 21%)]  Loss: 0.0235\n",
            "[25600/60000 ( 43%)]  Loss: 0.0310\n",
            "[38400/60000 ( 64%)]  Loss: 0.0043\n",
            "[51200/60000 ( 85%)]  Loss: 0.0114\n",
            "\n",
            "Average test loss: 0.0728  Accuracy: 9795/10000 (97.95%)\n",
            "\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64} Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0185\n",
            "[12800/60000 ( 21%)]  Loss: 0.0297\n",
            "[25600/60000 ( 43%)]  Loss: 0.0218\n",
            "[38400/60000 ( 64%)]  Loss: 0.0413\n",
            "[51200/60000 ( 85%)]  Loss: 0.0931\n",
            "\n",
            "Average test loss: 0.0755  Accuracy: 9797/10000 (97.97%)\n",
            "\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64} Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0025\n",
            "[12800/60000 ( 21%)]  Loss: 0.0051\n",
            "[25600/60000 ( 43%)]  Loss: 0.0455\n",
            "[38400/60000 ( 64%)]  Loss: 0.0496\n",
            "[51200/60000 ( 85%)]  Loss: 0.0551\n",
            "\n",
            "Average test loss: 0.0799  Accuracy: 9769/10000 (97.69%)\n",
            "\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64} Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0153\n",
            "[12800/60000 ( 21%)]  Loss: 0.0228\n",
            "[25600/60000 ( 43%)]  Loss: 0.0412\n",
            "[38400/60000 ( 64%)]  Loss: 0.0094\n",
            "[51200/60000 ( 85%)]  Loss: 0.0322\n",
            "\n",
            "Average test loss: 0.0719  Accuracy: 9800/10000 (98.00%)\n",
            "\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64} Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0060\n",
            "[12800/60000 ( 21%)]  Loss: 0.0269\n",
            "[25600/60000 ( 43%)]  Loss: 0.0241\n",
            "[38400/60000 ( 64%)]  Loss: 0.0095\n",
            "[51200/60000 ( 85%)]  Loss: 0.0318\n",
            "\n",
            "Average test loss: 0.0885  Accuracy: 9772/10000 (97.72%)\n",
            "\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64} Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0445\n",
            "[12800/60000 ( 21%)]  Loss: 0.0108\n",
            "[25600/60000 ( 43%)]  Loss: 0.0468\n",
            "[38400/60000 ( 64%)]  Loss: 0.0062\n",
            "[51200/60000 ( 85%)]  Loss: 0.0101\n",
            "\n",
            "Average test loss: 0.0794  Accuracy: 9787/10000 (97.87%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128} Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3574\n",
            "[12800/60000 ( 21%)]  Loss: 1.0581\n",
            "[25600/60000 ( 43%)]  Loss: 0.5221\n",
            "[38400/60000 ( 64%)]  Loss: 0.3098\n",
            "[51200/60000 ( 85%)]  Loss: 0.3491\n",
            "\n",
            "Average test loss: 0.2380  Accuracy: 9254/10000 (92.54%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128} Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.2382\n",
            "[12800/60000 ( 21%)]  Loss: 0.1968\n",
            "[25600/60000 ( 43%)]  Loss: 0.2623\n",
            "[38400/60000 ( 64%)]  Loss: 0.2920\n",
            "[51200/60000 ( 85%)]  Loss: 0.1480\n",
            "\n",
            "Average test loss: 0.1452  Accuracy: 9530/10000 (95.30%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128} Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.1297\n",
            "[12800/60000 ( 21%)]  Loss: 0.0848\n",
            "[25600/60000 ( 43%)]  Loss: 0.0974\n",
            "[38400/60000 ( 64%)]  Loss: 0.1567\n",
            "[51200/60000 ( 85%)]  Loss: 0.0811\n",
            "\n",
            "Average test loss: 0.1006  Accuracy: 9691/10000 (96.91%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128} Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.0824\n",
            "[12800/60000 ( 21%)]  Loss: 0.0704\n",
            "[25600/60000 ( 43%)]  Loss: 0.1612\n",
            "[38400/60000 ( 64%)]  Loss: 0.1008\n",
            "[51200/60000 ( 85%)]  Loss: 0.0899\n",
            "\n",
            "Average test loss: 0.0789  Accuracy: 9742/10000 (97.42%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128} Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0443\n",
            "[12800/60000 ( 21%)]  Loss: 0.0476\n",
            "[25600/60000 ( 43%)]  Loss: 0.0402\n",
            "[38400/60000 ( 64%)]  Loss: 0.0175\n",
            "[51200/60000 ( 85%)]  Loss: 0.0690\n",
            "\n",
            "Average test loss: 0.0910  Accuracy: 9710/10000 (97.10%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128} Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0603\n",
            "[12800/60000 ( 21%)]  Loss: 0.0990\n",
            "[25600/60000 ( 43%)]  Loss: 0.0300\n",
            "[38400/60000 ( 64%)]  Loss: 0.0664\n",
            "[51200/60000 ( 85%)]  Loss: 0.0649\n",
            "\n",
            "Average test loss: 0.1036  Accuracy: 9686/10000 (96.86%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128} Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0329\n",
            "[12800/60000 ( 21%)]  Loss: 0.0954\n",
            "[25600/60000 ( 43%)]  Loss: 0.0142\n",
            "[38400/60000 ( 64%)]  Loss: 0.0448\n",
            "[51200/60000 ( 85%)]  Loss: 0.0437\n",
            "\n",
            "Average test loss: 0.0724  Accuracy: 9776/10000 (97.76%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128} Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0503\n",
            "[12800/60000 ( 21%)]  Loss: 0.0250\n",
            "[25600/60000 ( 43%)]  Loss: 0.0243\n",
            "[38400/60000 ( 64%)]  Loss: 0.0366\n",
            "[51200/60000 ( 85%)]  Loss: 0.1278\n",
            "\n",
            "Average test loss: 0.0715  Accuracy: 9792/10000 (97.92%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128} Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0182\n",
            "[12800/60000 ( 21%)]  Loss: 0.0244\n",
            "[25600/60000 ( 43%)]  Loss: 0.0220\n",
            "[38400/60000 ( 64%)]  Loss: 0.0644\n",
            "[51200/60000 ( 85%)]  Loss: 0.0996\n",
            "\n",
            "Average test loss: 0.0721  Accuracy: 9779/10000 (97.79%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128} Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0282\n",
            "[12800/60000 ( 21%)]  Loss: 0.0604\n",
            "[25600/60000 ( 43%)]  Loss: 0.0848\n",
            "[38400/60000 ( 64%)]  Loss: 0.0126\n",
            "[51200/60000 ( 85%)]  Loss: 0.0058\n",
            "\n",
            "Average test loss: 0.0725  Accuracy: 9799/10000 (97.99%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128} Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0239\n",
            "[12800/60000 ( 21%)]  Loss: 0.0195\n",
            "[25600/60000 ( 43%)]  Loss: 0.0255\n",
            "[38400/60000 ( 64%)]  Loss: 0.0695\n",
            "[51200/60000 ( 85%)]  Loss: 0.0346\n",
            "\n",
            "Average test loss: 0.0637  Accuracy: 9821/10000 (98.21%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128} Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0039\n",
            "[12800/60000 ( 21%)]  Loss: 0.0400\n",
            "[25600/60000 ( 43%)]  Loss: 0.0156\n",
            "[38400/60000 ( 64%)]  Loss: 0.0179\n",
            "[51200/60000 ( 85%)]  Loss: 0.0349\n",
            "\n",
            "Average test loss: 0.0734  Accuracy: 9807/10000 (98.07%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128} Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0147\n",
            "[12800/60000 ( 21%)]  Loss: 0.0064\n",
            "[25600/60000 ( 43%)]  Loss: 0.0171\n",
            "[38400/60000 ( 64%)]  Loss: 0.0565\n",
            "[51200/60000 ( 85%)]  Loss: 0.0060\n",
            "\n",
            "Average test loss: 0.0654  Accuracy: 9836/10000 (98.36%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128} Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0178\n",
            "[12800/60000 ( 21%)]  Loss: 0.0013\n",
            "[25600/60000 ( 43%)]  Loss: 0.0156\n",
            "[38400/60000 ( 64%)]  Loss: 0.0007\n",
            "[51200/60000 ( 85%)]  Loss: 0.0429\n",
            "\n",
            "Average test loss: 0.0720  Accuracy: 9822/10000 (98.22%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128} Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0218\n",
            "[12800/60000 ( 21%)]  Loss: 0.0101\n",
            "[25600/60000 ( 43%)]  Loss: 0.0393\n",
            "[38400/60000 ( 64%)]  Loss: 0.0112\n",
            "[51200/60000 ( 85%)]  Loss: 0.0026\n",
            "\n",
            "Average test loss: 0.0670  Accuracy: 9813/10000 (98.13%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128} Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0072\n",
            "[12800/60000 ( 21%)]  Loss: 0.0016\n",
            "[25600/60000 ( 43%)]  Loss: 0.0049\n",
            "[38400/60000 ( 64%)]  Loss: 0.0065\n",
            "[51200/60000 ( 85%)]  Loss: 0.0091\n",
            "\n",
            "Average test loss: 0.0733  Accuracy: 9824/10000 (98.24%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128} Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0057\n",
            "[12800/60000 ( 21%)]  Loss: 0.0269\n",
            "[25600/60000 ( 43%)]  Loss: 0.0008\n",
            "[38400/60000 ( 64%)]  Loss: 0.0039\n",
            "[51200/60000 ( 85%)]  Loss: 0.0065\n",
            "\n",
            "Average test loss: 0.0664  Accuracy: 9854/10000 (98.54%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128} Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0034\n",
            "[12800/60000 ( 21%)]  Loss: 0.0002\n",
            "[25600/60000 ( 43%)]  Loss: 0.0014\n",
            "[38400/60000 ( 64%)]  Loss: 0.0002\n",
            "[51200/60000 ( 85%)]  Loss: 0.0242\n",
            "\n",
            "Average test loss: 0.0795  Accuracy: 9825/10000 (98.25%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128} Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0322\n",
            "[12800/60000 ( 21%)]  Loss: 0.0038\n",
            "[25600/60000 ( 43%)]  Loss: 0.0278\n",
            "[38400/60000 ( 64%)]  Loss: 0.0059\n",
            "[51200/60000 ( 85%)]  Loss: 0.0004\n",
            "\n",
            "Average test loss: 0.0714  Accuracy: 9832/10000 (98.32%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128} Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0036\n",
            "[12800/60000 ( 21%)]  Loss: 0.0106\n",
            "[25600/60000 ( 43%)]  Loss: 0.0001\n",
            "[38400/60000 ( 64%)]  Loss: 0.0024\n",
            "[51200/60000 ( 85%)]  Loss: 0.0092\n",
            "\n",
            "Average test loss: 0.0705  Accuracy: 9862/10000 (98.62%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128} Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3417\n",
            "[12800/60000 ( 21%)]  Loss: 0.5775\n",
            "[25600/60000 ( 43%)]  Loss: 0.2293\n",
            "[38400/60000 ( 64%)]  Loss: 0.2101\n",
            "[51200/60000 ( 85%)]  Loss: 0.2435\n",
            "\n",
            "Average test loss: 0.1396  Accuracy: 9561/10000 (95.61%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128} Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.1289\n",
            "[12800/60000 ( 21%)]  Loss: 0.1320\n",
            "[25600/60000 ( 43%)]  Loss: 0.1802\n",
            "[38400/60000 ( 64%)]  Loss: 0.1671\n",
            "[51200/60000 ( 85%)]  Loss: 0.1054\n",
            "\n",
            "Average test loss: 0.1084  Accuracy: 9659/10000 (96.59%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128} Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.0599\n",
            "[12800/60000 ( 21%)]  Loss: 0.1022\n",
            "[25600/60000 ( 43%)]  Loss: 0.0275\n",
            "[38400/60000 ( 64%)]  Loss: 0.1178\n",
            "[51200/60000 ( 85%)]  Loss: 0.0841\n",
            "\n",
            "Average test loss: 0.1069  Accuracy: 9646/10000 (96.46%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128} Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.0545\n",
            "[12800/60000 ( 21%)]  Loss: 0.0381\n",
            "[25600/60000 ( 43%)]  Loss: 0.0568\n",
            "[38400/60000 ( 64%)]  Loss: 0.0567\n",
            "[51200/60000 ( 85%)]  Loss: 0.0578\n",
            "\n",
            "Average test loss: 0.0957  Accuracy: 9696/10000 (96.96%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128} Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0391\n",
            "[12800/60000 ( 21%)]  Loss: 0.0318\n",
            "[25600/60000 ( 43%)]  Loss: 0.0904\n",
            "[38400/60000 ( 64%)]  Loss: 0.0800\n",
            "[51200/60000 ( 85%)]  Loss: 0.0504\n",
            "\n",
            "Average test loss: 0.0860  Accuracy: 9748/10000 (97.48%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128} Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0250\n",
            "[12800/60000 ( 21%)]  Loss: 0.0240\n",
            "[25600/60000 ( 43%)]  Loss: 0.0235\n",
            "[38400/60000 ( 64%)]  Loss: 0.0798\n",
            "[51200/60000 ( 85%)]  Loss: 0.0760\n",
            "\n",
            "Average test loss: 0.0763  Accuracy: 9755/10000 (97.55%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128} Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0397\n",
            "[12800/60000 ( 21%)]  Loss: 0.0182\n",
            "[25600/60000 ( 43%)]  Loss: 0.0190\n",
            "[38400/60000 ( 64%)]  Loss: 0.0273\n",
            "[51200/60000 ( 85%)]  Loss: 0.0215\n",
            "\n",
            "Average test loss: 0.0662  Accuracy: 9802/10000 (98.02%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128} Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0044\n",
            "[12800/60000 ( 21%)]  Loss: 0.0086\n",
            "[25600/60000 ( 43%)]  Loss: 0.0656\n",
            "[38400/60000 ( 64%)]  Loss: 0.0032\n",
            "[51200/60000 ( 85%)]  Loss: 0.0303\n",
            "\n",
            "Average test loss: 0.0576  Accuracy: 9827/10000 (98.27%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128} Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0031\n",
            "[12800/60000 ( 21%)]  Loss: 0.0118\n",
            "[25600/60000 ( 43%)]  Loss: 0.0434\n",
            "[38400/60000 ( 64%)]  Loss: 0.0299\n",
            "[51200/60000 ( 85%)]  Loss: 0.0092\n",
            "\n",
            "Average test loss: 0.0718  Accuracy: 9796/10000 (97.96%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128} Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0072\n",
            "[12800/60000 ( 21%)]  Loss: 0.0045\n",
            "[25600/60000 ( 43%)]  Loss: 0.0023\n",
            "[38400/60000 ( 64%)]  Loss: 0.0467\n",
            "[51200/60000 ( 85%)]  Loss: 0.0074\n",
            "\n",
            "Average test loss: 0.0659  Accuracy: 9797/10000 (97.97%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128} Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0208\n",
            "[12800/60000 ( 21%)]  Loss: 0.0124\n",
            "[25600/60000 ( 43%)]  Loss: 0.0247\n",
            "[38400/60000 ( 64%)]  Loss: 0.0127\n",
            "[51200/60000 ( 85%)]  Loss: 0.0031\n",
            "\n",
            "Average test loss: 0.0669  Accuracy: 9817/10000 (98.17%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128} Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0010\n",
            "[12800/60000 ( 21%)]  Loss: 0.0032\n",
            "[25600/60000 ( 43%)]  Loss: 0.0110\n",
            "[38400/60000 ( 64%)]  Loss: 0.0045\n",
            "[51200/60000 ( 85%)]  Loss: 0.0597\n",
            "\n",
            "Average test loss: 0.0711  Accuracy: 9794/10000 (97.94%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128} Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0345\n",
            "[12800/60000 ( 21%)]  Loss: 0.0053\n",
            "[25600/60000 ( 43%)]  Loss: 0.0352\n",
            "[38400/60000 ( 64%)]  Loss: 0.0069\n",
            "[51200/60000 ( 85%)]  Loss: 0.0642\n",
            "\n",
            "Average test loss: 0.0715  Accuracy: 9810/10000 (98.10%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128} Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0356\n",
            "[12800/60000 ( 21%)]  Loss: 0.0018\n",
            "[25600/60000 ( 43%)]  Loss: 0.0087\n",
            "[38400/60000 ( 64%)]  Loss: 0.0336\n",
            "[51200/60000 ( 85%)]  Loss: 0.0394\n",
            "\n",
            "Average test loss: 0.0655  Accuracy: 9828/10000 (98.28%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128} Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0158\n",
            "[12800/60000 ( 21%)]  Loss: 0.0008\n",
            "[25600/60000 ( 43%)]  Loss: 0.0106\n",
            "[38400/60000 ( 64%)]  Loss: 0.0128\n",
            "[51200/60000 ( 85%)]  Loss: 0.0191\n",
            "\n",
            "Average test loss: 0.0744  Accuracy: 9821/10000 (98.21%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128} Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0038\n",
            "[12800/60000 ( 21%)]  Loss: 0.0078\n",
            "[25600/60000 ( 43%)]  Loss: 0.0011\n",
            "[38400/60000 ( 64%)]  Loss: 0.0057\n",
            "[51200/60000 ( 85%)]  Loss: 0.0546\n",
            "\n",
            "Average test loss: 0.0605  Accuracy: 9855/10000 (98.55%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128} Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0020\n",
            "[12800/60000 ( 21%)]  Loss: 0.0024\n",
            "[25600/60000 ( 43%)]  Loss: 0.0223\n",
            "[38400/60000 ( 64%)]  Loss: 0.0257\n",
            "[51200/60000 ( 85%)]  Loss: 0.0162\n",
            "\n",
            "Average test loss: 0.0659  Accuracy: 9837/10000 (98.37%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128} Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0481\n",
            "[12800/60000 ( 21%)]  Loss: 0.0009\n",
            "[25600/60000 ( 43%)]  Loss: 0.0157\n",
            "[38400/60000 ( 64%)]  Loss: 0.0013\n",
            "[51200/60000 ( 85%)]  Loss: 0.0102\n",
            "\n",
            "Average test loss: 0.0633  Accuracy: 9861/10000 (98.61%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128} Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0132\n",
            "[12800/60000 ( 21%)]  Loss: 0.0026\n",
            "[25600/60000 ( 43%)]  Loss: 0.0017\n",
            "[38400/60000 ( 64%)]  Loss: 0.0037\n",
            "[51200/60000 ( 85%)]  Loss: 0.0010\n",
            "\n",
            "Average test loss: 0.0845  Accuracy: 9805/10000 (98.05%)\n",
            "\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128} Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0022\n",
            "[12800/60000 ( 21%)]  Loss: 0.0005\n",
            "[25600/60000 ( 43%)]  Loss: 0.0178\n",
            "[38400/60000 ( 64%)]  Loss: 0.0047\n",
            "[51200/60000 ( 85%)]  Loss: 0.0129\n",
            "\n",
            "Average test loss: 0.0637  Accuracy: 9848/10000 (98.48%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256} Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3739\n",
            "[12800/60000 ( 21%)]  Loss: 1.1636\n",
            "[25600/60000 ( 43%)]  Loss: 0.4793\n",
            "[38400/60000 ( 64%)]  Loss: 0.2448\n",
            "[51200/60000 ( 85%)]  Loss: 0.2753\n",
            "\n",
            "Average test loss: 0.1902  Accuracy: 9439/10000 (94.39%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256} Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.1880\n",
            "[12800/60000 ( 21%)]  Loss: 0.1097\n",
            "[25600/60000 ( 43%)]  Loss: 0.1502\n",
            "[38400/60000 ( 64%)]  Loss: 0.1676\n",
            "[51200/60000 ( 85%)]  Loss: 0.1418\n",
            "\n",
            "Average test loss: 0.1150  Accuracy: 9642/10000 (96.42%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256} Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.0933\n",
            "[12800/60000 ( 21%)]  Loss: 0.0442\n",
            "[25600/60000 ( 43%)]  Loss: 0.0501\n",
            "[38400/60000 ( 64%)]  Loss: 0.0864\n",
            "[51200/60000 ( 85%)]  Loss: 0.1614\n",
            "\n",
            "Average test loss: 0.1049  Accuracy: 9667/10000 (96.67%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256} Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.1187\n",
            "[12800/60000 ( 21%)]  Loss: 0.0741\n",
            "[25600/60000 ( 43%)]  Loss: 0.0555\n",
            "[38400/60000 ( 64%)]  Loss: 0.0233\n",
            "[51200/60000 ( 85%)]  Loss: 0.1711\n",
            "\n",
            "Average test loss: 0.0774  Accuracy: 9763/10000 (97.63%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256} Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.1243\n",
            "[12800/60000 ( 21%)]  Loss: 0.0298\n",
            "[25600/60000 ( 43%)]  Loss: 0.0257\n",
            "[38400/60000 ( 64%)]  Loss: 0.1022\n",
            "[51200/60000 ( 85%)]  Loss: 0.0440\n",
            "\n",
            "Average test loss: 0.0828  Accuracy: 9739/10000 (97.39%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256} Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0625\n",
            "[12800/60000 ( 21%)]  Loss: 0.0773\n",
            "[25600/60000 ( 43%)]  Loss: 0.0901\n",
            "[38400/60000 ( 64%)]  Loss: 0.0791\n",
            "[51200/60000 ( 85%)]  Loss: 0.0446\n",
            "\n",
            "Average test loss: 0.0753  Accuracy: 9777/10000 (97.77%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256} Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0651\n",
            "[12800/60000 ( 21%)]  Loss: 0.0044\n",
            "[25600/60000 ( 43%)]  Loss: 0.0808\n",
            "[38400/60000 ( 64%)]  Loss: 0.0701\n",
            "[51200/60000 ( 85%)]  Loss: 0.0588\n",
            "\n",
            "Average test loss: 0.0679  Accuracy: 9811/10000 (98.11%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256} Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0296\n",
            "[12800/60000 ( 21%)]  Loss: 0.0093\n",
            "[25600/60000 ( 43%)]  Loss: 0.0169\n",
            "[38400/60000 ( 64%)]  Loss: 0.0466\n",
            "[51200/60000 ( 85%)]  Loss: 0.0270\n",
            "\n",
            "Average test loss: 0.0695  Accuracy: 9790/10000 (97.90%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256} Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0547\n",
            "[12800/60000 ( 21%)]  Loss: 0.0764\n",
            "[25600/60000 ( 43%)]  Loss: 0.0466\n",
            "[38400/60000 ( 64%)]  Loss: 0.0080\n",
            "[51200/60000 ( 85%)]  Loss: 0.0505\n",
            "\n",
            "Average test loss: 0.0751  Accuracy: 9782/10000 (97.82%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256} Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0196\n",
            "[12800/60000 ( 21%)]  Loss: 0.0136\n",
            "[25600/60000 ( 43%)]  Loss: 0.0133\n",
            "[38400/60000 ( 64%)]  Loss: 0.0476\n",
            "[51200/60000 ( 85%)]  Loss: 0.0081\n",
            "\n",
            "Average test loss: 0.0669  Accuracy: 9815/10000 (98.15%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256} Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0355\n",
            "[12800/60000 ( 21%)]  Loss: 0.0195\n",
            "[25600/60000 ( 43%)]  Loss: 0.0032\n",
            "[38400/60000 ( 64%)]  Loss: 0.0077\n",
            "[51200/60000 ( 85%)]  Loss: 0.0215\n",
            "\n",
            "Average test loss: 0.0808  Accuracy: 9785/10000 (97.85%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256} Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0045\n",
            "[12800/60000 ( 21%)]  Loss: 0.0119\n",
            "[25600/60000 ( 43%)]  Loss: 0.0367\n",
            "[38400/60000 ( 64%)]  Loss: 0.0188\n",
            "[51200/60000 ( 85%)]  Loss: 0.0234\n",
            "\n",
            "Average test loss: 0.0668  Accuracy: 9817/10000 (98.17%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256} Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0155\n",
            "[12800/60000 ( 21%)]  Loss: 0.0153\n",
            "[25600/60000 ( 43%)]  Loss: 0.0067\n",
            "[38400/60000 ( 64%)]  Loss: 0.0264\n",
            "[51200/60000 ( 85%)]  Loss: 0.0077\n",
            "\n",
            "Average test loss: 0.0734  Accuracy: 9823/10000 (98.23%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256} Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0028\n",
            "[12800/60000 ( 21%)]  Loss: 0.0217\n",
            "[25600/60000 ( 43%)]  Loss: 0.0029\n",
            "[38400/60000 ( 64%)]  Loss: 0.0125\n",
            "[51200/60000 ( 85%)]  Loss: 0.0005\n",
            "\n",
            "Average test loss: 0.0664  Accuracy: 9826/10000 (98.26%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256} Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0021\n",
            "[12800/60000 ( 21%)]  Loss: 0.0037\n",
            "[25600/60000 ( 43%)]  Loss: 0.0047\n",
            "[38400/60000 ( 64%)]  Loss: 0.0052\n",
            "[51200/60000 ( 85%)]  Loss: 0.0038\n",
            "\n",
            "Average test loss: 0.0862  Accuracy: 9782/10000 (97.82%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256} Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0773\n",
            "[12800/60000 ( 21%)]  Loss: 0.0459\n",
            "[25600/60000 ( 43%)]  Loss: 0.0084\n",
            "[38400/60000 ( 64%)]  Loss: 0.0038\n",
            "[51200/60000 ( 85%)]  Loss: 0.0041\n",
            "\n",
            "Average test loss: 0.0736  Accuracy: 9824/10000 (98.24%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256} Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0032\n",
            "[12800/60000 ( 21%)]  Loss: 0.0004\n",
            "[25600/60000 ( 43%)]  Loss: 0.0031\n",
            "[38400/60000 ( 64%)]  Loss: 0.0314\n",
            "[51200/60000 ( 85%)]  Loss: 0.0030\n",
            "\n",
            "Average test loss: 0.0707  Accuracy: 9840/10000 (98.40%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256} Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0061\n",
            "[12800/60000 ( 21%)]  Loss: 0.0005\n",
            "[25600/60000 ( 43%)]  Loss: 0.0036\n",
            "[38400/60000 ( 64%)]  Loss: 0.0134\n",
            "[51200/60000 ( 85%)]  Loss: 0.0058\n",
            "\n",
            "Average test loss: 0.0783  Accuracy: 9833/10000 (98.33%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256} Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0043\n",
            "[12800/60000 ( 21%)]  Loss: 0.0149\n",
            "[25600/60000 ( 43%)]  Loss: 0.0048\n",
            "[38400/60000 ( 64%)]  Loss: 0.0120\n",
            "[51200/60000 ( 85%)]  Loss: 0.0004\n",
            "\n",
            "Average test loss: 0.0791  Accuracy: 9818/10000 (98.18%)\n",
            "\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256} Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0004\n",
            "[12800/60000 ( 21%)]  Loss: 0.0161\n",
            "[25600/60000 ( 43%)]  Loss: 0.0013\n",
            "[38400/60000 ( 64%)]  Loss: 0.0059\n",
            "[51200/60000 ( 85%)]  Loss: 0.0013\n",
            "\n",
            "Average test loss: 0.0666  Accuracy: 9839/10000 (98.39%)\n",
            "\n",
            "Config: {'dim': 96, 'depth': 7, 'heads': 10, 'mlp_dim': 192} Epoch: 1 LR: [0.001]\n"
          ]
        },
        {
          "ename": "EinopsError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/einops/einops.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0mrecipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_transformation_recipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         return _apply_recipe(\n\u001b[0m\u001b[1;32m    524\u001b[0m             \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes_lengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhashable_axes_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/einops/einops.py\u001b[0m in \u001b[0;36m_apply_recipe\u001b[0;34m(backend, recipe, tensor, reduction_type, axes_lengths)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mrecipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/einops/einops.py\u001b[0m in \u001b[0;36m_reconstruct_from_shape_uncached\u001b[0;34m(self, shape, axes_dims)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknown_product\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mknown_product\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEinopsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEinopsError\u001b[0m: Shape mismatch, can't divide axis of length 288 in chunks of 30",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-3d6d9ae76d9d>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mexecution_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecution_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-e6c8f5494b1f>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_EPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Config:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Epoch:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LR:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_last_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-b19ace1e5e64>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, data_loader, loss_history)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-8226d14d8a3c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, mask)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_cls_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-8226d14d8a3c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mff\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-8226d14d8a3c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPreNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-8226d14d8a3c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mFeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-8226d14d8a3c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mqkv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_qkv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b n (qkv h d) -> qkv b h n d'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqkv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mdots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bhid,bhjd->bhij'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/einops/einops.py\u001b[0m in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \"\"\"\n\u001b[0;32m--> 591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rearrange\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0maxes_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/einops/einops.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n Input is list. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Additional info: {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mEinopsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEinopsError\u001b[0m:  Error while processing rearrange-reduction pattern \"b n (qkv h d) -> qkv b h n d\".\n Input tensor shape: torch.Size([128, 17, 288]). Additional info: {'qkv': 3, 'h': 10}.\n Shape mismatch, can't divide axis of length 288 in chunks of 30"
          ]
        }
      ],
      "source": [
        "# List of configurations to try\n",
        "# configs = [\n",
        "#     {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128},  # Baseline\n",
        "#     {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256},  # Increased Capacity\n",
        "#     # Add other configurations here\n",
        "# ]\n",
        "\n",
        "configs = [\n",
        "    # Baseline Model\n",
        "    {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128},  # Original setup as a baseline\n",
        "\n",
        "    # Increased Capacity Model\n",
        "    {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256},  # Increases capacity for better feature learning but risks overfitting\n",
        "\n",
        "    # Reduced Capacity Model\n",
        "    {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64},  # Smaller model, potentially faster training and better generalization\n",
        "\n",
        "    # Increased Depth\n",
        "    {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128},  # More layers for complex features, but more challenging to train\n",
        "\n",
        "    # Fewer Heads with Higher Dimension\n",
        "    {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128},  # Higher dimension per head with fewer heads, affecting attention diversity\n",
        "\n",
        "    # High MLP Dimension\n",
        "    {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256},  # Increased MLP dimension for potentially improved complex function learning\n",
        "\n",
        "    # Balanced Increase\n",
        "    {'dim': 96, 'depth': 7, 'heads': 10, 'mlp_dim': 192}  # Moderately larger model with balanced increase across parameters\n",
        "]\n",
        "\n",
        "\n",
        "results = []\n",
        "for config in configs:\n",
        "    execution_time, accuracy = train_and_evaluate(config)\n",
        "    results.append((config, execution_time, accuracy))\n",
        "\n",
        "# Print results\n",
        "for config, execution_time, accuracy in results:\n",
        "    print(f\"Config: {config}, Time: {execution_time:.2f} sec, Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVdlEgheyJTe",
        "outputId": "499983b2-cadd-496f-bd1e-fdae83653f8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 128}, Time: 293.55 sec, Accuracy: 98.43%\n",
            "Config: {'dim': 128, 'depth': 8, 'heads': 16, 'mlp_dim': 256}, Time: 332.59 sec, Accuracy: 98.67%\n",
            "Config: {'dim': 32, 'depth': 4, 'heads': 4, 'mlp_dim': 64}, Time: 250.83 sec, Accuracy: 97.87%\n",
            "Config: {'dim': 64, 'depth': 12, 'heads': 8, 'mlp_dim': 128}, Time: 421.16 sec, Accuracy: 98.62%\n",
            "Config: {'dim': 128, 'depth': 6, 'heads': 4, 'mlp_dim': 128}, Time: 298.24 sec, Accuracy: 98.48%\n",
            "Config: {'dim': 64, 'depth': 6, 'heads': 8, 'mlp_dim': 256}, Time: 297.36 sec, Accuracy: 98.39%\n"
          ]
        }
      ],
      "source": [
        "# Print results\n",
        "for config, execution_time, accuracy in results:\n",
        "    print(f\"Config: {config}, Time: {execution_time:.2f} sec, Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUyIalvG6jLv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
