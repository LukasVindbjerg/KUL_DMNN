{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G28ynkon-QY1"
      },
      "source": [
        "#DATA MINING AND NEURAL NETWORKS    \n",
        "##Assignment 3.2 - Autoencoders and Stacked Autoencoders\n",
        "\n",
        "In this file, we will implement two network architectures from scrath: (1) Autoencoders; (2) Stacked Autoencoders.\n",
        "\n",
        "We will train both networks on the MNIST dataset under reconstruction learning task. All training will be conducted on a single T4 GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZHATEBH-INR",
        "outputId": "5fca9600-d04c-464a-aae8-4b8e82cc2447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Please first load your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYPTjNQd-KRb",
        "outputId": "12943aaf-a2e4-4ffb-e645-c51afa3dcc0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: cd: /content/drive/MyDrive/DMNN/DMNN2023: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Please go to the folder with all assignment files\n",
        "# Please change the following path to your own path\n",
        "!cd /content/drive/MyDrive/DMNN/DMNN2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnZZO_oOFpJP",
        "outputId": "a0c19ae7-3160-4773-9a0b-180985f41cf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Dec 19 17:36:23 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P0              28W /  70W |    183MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Please go to Edit > Notebook settings > Hardware accelerator > choose \"T4 GPU\"\n",
        "# Now check if you have loaded the GPU successfully\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "504PW8Go_Bna"
      },
      "source": [
        "# Autoencoder\n",
        "In this section, we implement Linear Autoencoder from scratch and training it on the MNIST dataset. With this autoencoder, we pass input data through an encoder making a compressed representation of the input, and then pass this representation to the following decoder to build the reconstruction data.\n",
        "Codes will be implemeneted with ``PyTorch``.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGwR7aL5_FSD",
        "outputId": "fd703af0-9720-42f4-f277-fffdda428ae7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 433120930.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 25718830.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 184556651.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 23147665.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Load the training and test datasets\n",
        "train_data = datasets.MNIST(root='data', train=True,\n",
        "                                   download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='data', train=False,\n",
        "                                  download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lig-NUzmAuiZ"
      },
      "outputs": [],
      "source": [
        "## Create training and test dataloaders\n",
        "\n",
        "# How many samples per batch to load\n",
        "# You can tune the batch size\n",
        "batch_size = 20\n",
        "\n",
        "# Prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle = True,\n",
        "                                           num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(test_data,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle = False,\n",
        "                                          num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk4UCmCdBjjZ"
      },
      "source": [
        "## Visualize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "3BeTdD4yBcl9",
        "outputId": "94caa424-e1b5-41bc-bd39-4b7a61ffc45f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7871c2034430>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "images = images.numpy()\n",
        "\n",
        "# Get one image from the batch\n",
        "img = np.squeeze(images[0])\n",
        "\n",
        "fig = plt.figure(figsize = (5,5))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(img, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzWxOoXYCUiz"
      },
      "source": [
        "## Build the Linear Autoencoder\n",
        "We now train a linear autoencoder on MINIST dataset.\n",
        "Images of original size 28$\\times$28 will be flattened into 784-dimensional vectors.\n",
        "Note that images from this dataset are already normalized so that the values are between 0 and 1.\n",
        "Since the images are normalized between 0 and 1, we need to use a sigmoid activation on the output layer to get values that match this input value range.\n",
        "\n",
        "The encoder and decoder in the Autoencoder are built with one linear layer where you can tune the dimension of the hidden representation, i.e., ``encoding_dim``, to obtain models with different size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05JucJsoB6E-"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the NN architecture\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, encoding_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Linear(784, encoding_dim)\n",
        "        # Decoder\n",
        "        self.decoder = nn.Linear(encoding_dim, 784)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define feedforward behavior\n",
        "        # and scale the *output* layer with a sigmoid activation function\n",
        "\n",
        "        # Pass x into encoder\n",
        "        out = F.relu(self.encoder(x))\n",
        "        # Pass out into decoder\n",
        "        out = torch.sigmoid(self.decoder(out))\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtBYi-kACg1A",
        "outputId": "fda384e8-33be-4bd1-b80d-7bf65ed426eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autoencoder(\n",
            "  (encoder): Linear(in_features=784, out_features=64, bias=True)\n",
            "  (decoder): Linear(in_features=64, out_features=784, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Initialize the NN\n",
        "# You can change the encoding_dim to obtain models with different size\n",
        "encoding_dim = 64\n",
        "model = Autoencoder(encoding_dim)\n",
        "\n",
        "# Send model to GPU\n",
        "model.cuda()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6oAhmIwEDgo"
      },
      "source": [
        "## Training on MNIST\n",
        "Since we work on reconstruction learning tasks, we do not need the labels here but only the images.\n",
        "The loss function should choose the MSE loss for reconstruction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHDQeQlfCh9P"
      },
      "outputs": [],
      "source": [
        "# Specify the loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# We use Adam as the optimizer with a fixed learning rate of 1e-3\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssUxyRJSEUzV",
        "outputId": "f1e6361e-045a-407c-e3bc-1553207332c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 \tTraining Loss: 0.426306\n",
            "Epoch: 1 \tTraining Loss: 0.136120\n",
            "Epoch: 2 \tTraining Loss: 0.115453\n",
            "Epoch: 3 \tTraining Loss: 0.109832\n",
            "Epoch: 4 \tTraining Loss: 0.106484\n",
            "Epoch: 5 \tTraining Loss: 0.104240\n",
            "Epoch: 6 \tTraining Loss: 0.102618\n",
            "Epoch: 7 \tTraining Loss: 0.101598\n",
            "Epoch: 8 \tTraining Loss: 0.100714\n",
            "Epoch: 9 \tTraining Loss: 0.099932\n",
            "Epoch: 10 \tTraining Loss: 0.099416\n",
            "Epoch: 11 \tTraining Loss: 0.098896\n",
            "Epoch: 12 \tTraining Loss: 0.098574\n",
            "Epoch: 13 \tTraining Loss: 0.098262\n",
            "Epoch: 14 \tTraining Loss: 0.097897\n",
            "Epoch: 15 \tTraining Loss: 0.097536\n",
            "Epoch: 16 \tTraining Loss: 0.097363\n",
            "Epoch: 17 \tTraining Loss: 0.097119\n",
            "Epoch: 18 \tTraining Loss: 0.096967\n",
            "Epoch: 19 \tTraining Loss: 0.096753\n"
          ]
        }
      ],
      "source": [
        "# Number of epochs to train the model\n",
        "# You can also tune the number of Epochs\n",
        "n_epochs = 20\n",
        "# Set model to training mode\n",
        "model.train()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Monitor training loss\n",
        "    train_loss = 0.0\n",
        "\n",
        "    # Train the model #\n",
        "\n",
        "    for data in train_loader:\n",
        "        # _ stands in for labels\n",
        "        # we do not need labels when conducting reconstruction\n",
        "        images, _ = data\n",
        "        # Flatten images and send images to GPU\n",
        "        images = images.view(images.size(0), -1).cuda()\n",
        "        # Clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "        outputs = model(images)\n",
        "        # Calculate the loss\n",
        "        loss = criterion(outputs, images)\n",
        "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # Perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # Update running training loss\n",
        "        train_loss += loss.item()*images.size(0)\n",
        "\n",
        "    # Print avg training statistics\n",
        "    train_loss = train_loss/len(train_loader)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
        "        epoch,\n",
        "        train_loss\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeN_f28fJ1lq"
      },
      "source": [
        "## Evaluation on test set\n",
        "We now evaluate the reconstruction results on the test set.\n",
        "We plot the original test images and their corresponding reconstruction ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "gemBn0ihG2Yo",
        "outputId": "ebdd3fe7-bd36-44cd-ebff-a7dc6dd619ac"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2500x400 with 20 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "images_flatten = images.view(images.size(0), -1)\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "# Send model back to CPU\n",
        "model.cpu()\n",
        "# Get sample outputs\n",
        "output = model(images_flatten)\n",
        "# Prep images for display\n",
        "images = images.numpy()\n",
        "\n",
        "# Output is resized into a batch of images\n",
        "output = output.view(batch_size, 1, 28, 28)\n",
        "# Use detach when it's an output that requires_grad\n",
        "output = output.detach().numpy()\n",
        "\n",
        "# Plot the first ten input images and then reconstructed images\n",
        "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
        "\n",
        "# Input images on top row, reconstructions on bottom\n",
        "for images, row in zip([images, output], axes):\n",
        "    for img, ax in zip(images, row):\n",
        "        ax.imshow(np.squeeze(img), cmap='gray')\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVYXhsPrLBrV"
      },
      "source": [
        "# Stacked Autoencoder\n",
        "In this section, we implement the Stacked Autoencoder from scrath and train it on the MNIST dataset.\n",
        "The training process is with two steps: (1) each layer is trained separately; (2) the network is trained as a whole.\n",
        "\n",
        "We first load the dataset as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8163o-y3SLfw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim, functional, utils\n",
        "from torch.nn import BCELoss\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, utils\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "import time, os\n",
        "\n",
        "def get_mnist_loader(batch_size=100, shuffle=True):\n",
        "    \"\"\"\n",
        "\n",
        "    :return: train_loader, test_loader\n",
        "    \"\"\"\n",
        "    train_dataset = MNIST(root='../data',\n",
        "                          train=True,\n",
        "                          transform=torchvision.transforms.ToTensor(),\n",
        "                          download=True)\n",
        "    test_dataset = MNIST(root='../data',\n",
        "                         train=False,\n",
        "                         transform=torchvision.transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=shuffle)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GimciNoQI73"
      },
      "source": [
        "## Build the Stacked Autoencoder\n",
        "Before building the Stacked Autoencoder, we need to build the Autoencoder layer first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNjUD_jNLDN3"
      },
      "outputs": [],
      "source": [
        "class AutoEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    fully-connected linear layers for stacked autoencoders.\n",
        "    This module can automatically be trained when training each layer is enabled\n",
        "    Yes, this is much like the simplest auto-encoder\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=None, output_dim=None, SelfTraining=False):\n",
        "        super(AutoEncoderLayer, self).__init__()\n",
        "        # If input_dim is None or output_dim is None:\n",
        "        # raise ValueError\n",
        "        self.in_features = input_dim\n",
        "        self.out_features = output_dim\n",
        "        # Whether to conduct layer-by-layer pre-training, or train the entire network\n",
        "        self.is_training_self = SelfTraining\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(self.in_features, self.out_features, bias=True),\n",
        "            nn.Sigmoid()  # use Sigmoid activation function\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.out_features, self.in_features, bias=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.encoder(x)\n",
        "        if self.is_training_self:\n",
        "            return self.decoder(out)\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "    def lock_grad(self):\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def acquire_grad(self):\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    @property\n",
        "    def input_dim(self):\n",
        "        return self.in_features\n",
        "\n",
        "    @property\n",
        "    def output_dim(self):\n",
        "        return self.out_features\n",
        "\n",
        "    @property\n",
        "    def is_training_layer(self):\n",
        "        return self.is_training_self\n",
        "\n",
        "    @is_training_layer.setter\n",
        "    def is_training_layer(self, other: bool):\n",
        "        self.is_training_self = other\n",
        "\n",
        "\n",
        "class StackedAutoEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Construct the whole network with layers_list\n",
        "    Stacked autoencoders is generally symmetrical about the middle hidden layer\n",
        "    \"\"\"\n",
        "    def __init__(self, layers_list=None):\n",
        "        super(StackedAutoEncoder, self).__init__()\n",
        "        self.layers_list = layers_list\n",
        "        self.initialize()\n",
        "        self.encoder_1 = self.layers_list[0]\n",
        "        self.encoder_2 = self.layers_list[1]\n",
        "        self.encoder_3 = self.layers_list[2]\n",
        "        self.encoder_4 = self.layers_list[3]\n",
        "\n",
        "    def initialize(self):\n",
        "        for layer in self.layers_list:\n",
        "            # assert isinstance(layer, AutoEncoderLayer)\n",
        "            layer.is_training_layer = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        out = self.encoder_1(out)\n",
        "        out = self.encoder_2(out)\n",
        "        out = self.encoder_3(out)\n",
        "        out = self.encoder_4(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UQL0TzMRxc3"
      },
      "source": [
        "## The training function of each layer\n",
        "We need to freeze the parameters in the previsou layers when training the current layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvZYBFsGRsmg"
      },
      "outputs": [],
      "source": [
        "def train_layers(layers_list=None, layer=None, epoch=None, validate=True):\n",
        "    \"\"\"\n",
        "    Greedy layer-wise training: when training the i-th layer, freeze the i-1 layer\n",
        "    :param layers_list:\n",
        "    :param layer:\n",
        "    :param epoch:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        for model in layers_list:\n",
        "            model.cuda()\n",
        "\n",
        "    train_loader, test_loader = get_mnist_loader(batch_size=batch_size, shuffle=True)\n",
        "    optimizer = optim.Adam(layers_list[layer].parameters(), lr=0.001)\n",
        "    criterion = BCELoss()\n",
        "\n",
        "    # Train\n",
        "    for epoch_index in range(epoch):\n",
        "        sum_loss = 0.\n",
        "\n",
        "        # Freeze the parameters of all layers before the current layer\n",
        "        # Layer 0 has no previous layer\n",
        "        if layer != 0:\n",
        "            for index in range(layer):\n",
        "                # In addition to freezing parameters\n",
        "                # the output return method of the frozen layer must also be set.\n",
        "                layers_list[index].lock_grad()\n",
        "                layers_list[index].is_training_layer = False\n",
        "\n",
        "        for batch_index, (train_data, _) in enumerate(train_loader):\n",
        "            # Generate input data\n",
        "            if torch.cuda.is_available():\n",
        "                train_data = train_data.cuda()  # Put data onto GPU\n",
        "            out = train_data.view(train_data.size(0), -1)\n",
        "\n",
        "            # Perform forward calculation on the frozen layers before (layer-1)-th layer\n",
        "            if layer != 0:\n",
        "                for l in range(layer):\n",
        "                    out = layers_list[l](out)\n",
        "\n",
        "            # Train the layer-th layer\n",
        "            pred = layers_list[layer](out)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(pred, out)\n",
        "            sum_loss += loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if (batch_index + 1) % 10 == 0:\n",
        "                print(\"Train Layer: {}, Epoch: {}/{}, Iter: {}/{}, Loss: {:.4f}\".format(\n",
        "                    layer, (epoch_index + 1), epoch, (batch_index + 1), len(train_loader), loss\n",
        "                ))\n",
        "\n",
        "\n",
        "        # Save reconstructed images after each epoch\n",
        "        test_data, _ = next(iter(test_loader))\n",
        "        with torch.no_grad():\n",
        "            layers_list[layer].eval()\n",
        "            test_data = test_data.cuda() if torch.cuda.is_available() else test_data\n",
        "            test_output = layers_list[layer](test_data.view(test_data.size(0), -1))\n",
        "            torchvision.utils.save_image(test_output.view(batch_size, 1, 28, 28), './test_images/pretrain_layer_{}_epoch_{}_output.png'.format(layer, epoch_index))\n",
        "\n",
        "\n",
        "        if validate:\n",
        "            pass\n",
        "\n",
        "# def train_layers(layers_list=None, layer=None, epoch=None, validate=True):\n",
        "#     \"\"\"\n",
        "#     Greedy layer-wise training: when training the i-th layer, freeze the i-1 layer\n",
        "#     :param layers_list:\n",
        "#     :param layer:\n",
        "#     :param epoch:\n",
        "#     :return:\n",
        "#     \"\"\"\n",
        "#     if torch.cuda.is_available():\n",
        "#         for model in layers_list:\n",
        "#             model.cuda()\n",
        "\n",
        "#     train_loader, test_loader = get_mnist_loader(batch_size=batch_size, shuffle=True)\n",
        "#     optimizer = optim.Adam(layers_list[layer].parameters(), lr=0.001)\n",
        "#     criterion = BCELoss()\n",
        "\n",
        "#     # Train\n",
        "#     for epoch_index in range(epoch):\n",
        "#         sum_loss = 0.\n",
        "\n",
        "#         # Freeze the parameters of all layers before the current layer\n",
        "#         # Layer 0 has no previous layer\n",
        "#         if layer != 0:\n",
        "#             for index in range(layer):\n",
        "#                 layers_list[index].lock_grad()\n",
        "#                 layers_list[index].is_training_layer = False\n",
        "\n",
        "#         for batch_index, (train_data, _) in enumerate(train_loader):\n",
        "#             if torch.cuda.is_available():\n",
        "#                 train_data = train_data.cuda()\n",
        "#             out = train_data.view(train_data.size(0), -1)\n",
        "\n",
        "#             if layer != 0:\n",
        "#                 for l in range(layer):\n",
        "#                     out = layers_list[l](out)\n",
        "\n",
        "#             pred = layers_list[layer](out)\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             loss = criterion(pred, out)\n",
        "#             sum_loss += loss\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             if (batch_index + 1) % 10 == 0:\n",
        "#                 print(\"Train Layer: {}, Epoch: {}/{}, Iter: {}/{}, Loss: {:.4f}\".format(\n",
        "#                     layer, (epoch_index + 1), epoch, (batch_index + 1), len(train_loader), loss\n",
        "#                 ))\n",
        "\n",
        "#         # Saving reconstructed images after each epoch\n",
        "#         test_data, _ = next(iter(test_loader))\n",
        "#         with torch.no_grad():\n",
        "#             if torch.cuda.is_available():\n",
        "#                 test_data = test_data.cuda()\n",
        "#             x = test_data.view(test_data.size(0), -1)\n",
        "\n",
        "#             if layer != 0:\n",
        "#                 for l in range(layer):\n",
        "#                     x = layers_list[l](x)\n",
        "\n",
        "#             reconstructed = layers_list[layer](x)\n",
        "#             image_tensor = reconstructed.view(batch_size, 1, 28, 28)\n",
        "#             torchvision.utils.save_image(image_tensor, './test_images/pretrain_layer_{}_epoch_{}_output.png'.format(layer, epoch_index))\n",
        "\n",
        "#         if validate:\n",
        "#             pass\n",
        "#             # Validation code can be added here if needed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzOYc3BWx1WY"
      },
      "source": [
        "## The training function of the whole network\n",
        "Now we unfreeze all the parameters in the network that are previously frozen for the layer training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3NM4lrWSrdS"
      },
      "outputs": [],
      "source": [
        "def train_whole(model=None, epoch=50, validate=True):\n",
        "\n",
        "    print(\">> start training whole model\")\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "\n",
        "    # unfreeze the parameters frozen in pre-training\n",
        "    for param in model.parameters():\n",
        "        param.require_grad = True\n",
        "\n",
        "    train_loader, test_loader = get_mnist_loader(batch_size=batch_size, shuffle=shuffle)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    # Generate/save the original test image --take a batch_size\n",
        "    test_data, _ = next(iter(test_loader))\n",
        "    torchvision.utils.save_image(test_data, './test_images/real_test_images.png')\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # train\n",
        "    for epoch_index in range(epoch):\n",
        "        sum_loss = 0.\n",
        "        for batch_index, (train_data, _) in enumerate(train_loader):\n",
        "            if torch.cuda.is_available():\n",
        "                train_data = train_data.cuda()\n",
        "            x = train_data.view(train_data.size(0), -1)\n",
        "\n",
        "            out = model(x)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(out, x)\n",
        "            sum_loss += loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (batch_index + 1) % 10 == 0:\n",
        "                print(\"Train Whole, Epoch: {}/{}, Iter: {}/{}, Loss: {:.4f}\".format(\n",
        "                    (epoch_index + 1), epoch, (batch_index + 1), len(train_loader), loss\n",
        "                ))\n",
        "            if batch_index == len(train_loader) - 1:\n",
        "                torchvision.utils.save_image(out.view(100, 1, 28, 28), \"./test_images/out_{}_{}.png\".format(epoch_index, batch_index))\n",
        "\n",
        "        # evaluate the model for each epoch\n",
        "        if validate:\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                if torch.cuda.is_available():\n",
        "                    test_data = test_data.cuda()\n",
        "                x = test_data.view(test_data.size(0), -1)\n",
        "                out = model(x)\n",
        "                loss = criterion(out, x)\n",
        "                print(\"Test Epoch: {}/{}, Iter: {}/{}, test Loss: {}\".format(\n",
        "                    epoch_index + 1, epoch, (epoch_index + 1), len(test_loader), loss\n",
        "                ))\n",
        "                image_tensor = out.view(batch_size, 1, 28, 28)\n",
        "                torchvision.utils.save_image(image_tensor, './test_images/test_image_epoch_{}.png'.format(epoch_index))\n",
        "\n",
        "    print(\"<< end training whole model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkccSfRJx4ah"
      },
      "source": [
        "## Let's start training!\n",
        "The default network is trained with layer epochs 20, whole epochs 50, batch size of 100. You can try to change the hyper-parameters to obtain better reconstruction performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm44Cap4T7oS"
      },
      "outputs": [],
      "source": [
        "# You can change the hyper-parameters here\n",
        "num_tranin_layer_epochs = 5 # 20\n",
        "num_tranin_whole_epochs = 5 # 50\n",
        "batch_size = 100\n",
        "shuffle = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M6N2XhTqT_-X",
        "outputId": "e6958f6a-b71d-4d5a-99ad-1cf57d14c49b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Layer: 0, Epoch: 1/5, Iter: 10/600, Loss: 0.3801\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 20/600, Loss: 0.2781\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 30/600, Loss: 0.2588\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 40/600, Loss: 0.2698\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 50/600, Loss: 0.2664\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 60/600, Loss: 0.2527\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 70/600, Loss: 0.2622\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 80/600, Loss: 0.2592\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 90/600, Loss: 0.2476\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 100/600, Loss: 0.2419\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 110/600, Loss: 0.2469\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 120/600, Loss: 0.2372\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 130/600, Loss: 0.2309\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 140/600, Loss: 0.2375\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 150/600, Loss: 0.2183\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 160/600, Loss: 0.2230\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 170/600, Loss: 0.2171\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 180/600, Loss: 0.2044\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 190/600, Loss: 0.1995\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 200/600, Loss: 0.2066\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 210/600, Loss: 0.1945\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 220/600, Loss: 0.1945\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 230/600, Loss: 0.2019\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 240/600, Loss: 0.2008\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 250/600, Loss: 0.1928\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 260/600, Loss: 0.1854\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 270/600, Loss: 0.1783\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 280/600, Loss: 0.1872\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 290/600, Loss: 0.1880\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 300/600, Loss: 0.1796\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 310/600, Loss: 0.1807\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 320/600, Loss: 0.1761\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 330/600, Loss: 0.1874\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 340/600, Loss: 0.1742\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 350/600, Loss: 0.1658\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 360/600, Loss: 0.1707\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 370/600, Loss: 0.1705\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 380/600, Loss: 0.1651\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 390/600, Loss: 0.1623\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 400/600, Loss: 0.1620\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 410/600, Loss: 0.1670\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 420/600, Loss: 0.1582\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 430/600, Loss: 0.1573\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 440/600, Loss: 0.1598\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 450/600, Loss: 0.1681\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 460/600, Loss: 0.1506\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 470/600, Loss: 0.1573\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 480/600, Loss: 0.1565\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 490/600, Loss: 0.1502\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 500/600, Loss: 0.1573\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 510/600, Loss: 0.1505\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 520/600, Loss: 0.1445\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 530/600, Loss: 0.1422\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 540/600, Loss: 0.1504\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 550/600, Loss: 0.1424\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 560/600, Loss: 0.1441\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 570/600, Loss: 0.1509\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 580/600, Loss: 0.1485\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 590/600, Loss: 0.1459\n",
            "Train Layer: 0, Epoch: 1/5, Iter: 600/600, Loss: 0.1416\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 10/600, Loss: 0.1364\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 20/600, Loss: 0.1371\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 30/600, Loss: 0.1346\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 40/600, Loss: 0.1424\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 50/600, Loss: 0.1351\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 60/600, Loss: 0.1277\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 70/600, Loss: 0.1319\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 80/600, Loss: 0.1348\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 90/600, Loss: 0.1306\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 100/600, Loss: 0.1296\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 110/600, Loss: 0.1345\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 120/600, Loss: 0.1346\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 130/600, Loss: 0.1320\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 140/600, Loss: 0.1290\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 150/600, Loss: 0.1275\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 160/600, Loss: 0.1358\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 170/600, Loss: 0.1246\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 180/600, Loss: 0.1257\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 190/600, Loss: 0.1315\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 200/600, Loss: 0.1258\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 210/600, Loss: 0.1245\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 220/600, Loss: 0.1231\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 230/600, Loss: 0.1198\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 240/600, Loss: 0.1220\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 250/600, Loss: 0.1235\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 260/600, Loss: 0.1239\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 270/600, Loss: 0.1269\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 280/600, Loss: 0.1199\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 290/600, Loss: 0.1275\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 300/600, Loss: 0.1162\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 310/600, Loss: 0.1214\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 320/600, Loss: 0.1150\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 330/600, Loss: 0.1195\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 340/600, Loss: 0.1135\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 350/600, Loss: 0.1106\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 360/600, Loss: 0.1173\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 370/600, Loss: 0.1145\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 380/600, Loss: 0.1132\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 390/600, Loss: 0.1146\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 400/600, Loss: 0.1142\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 410/600, Loss: 0.1062\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 420/600, Loss: 0.1094\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 430/600, Loss: 0.1133\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 440/600, Loss: 0.1110\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 450/600, Loss: 0.1140\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 460/600, Loss: 0.1108\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 470/600, Loss: 0.1089\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 480/600, Loss: 0.1101\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 490/600, Loss: 0.1077\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 500/600, Loss: 0.1080\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 510/600, Loss: 0.1077\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 520/600, Loss: 0.1076\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 530/600, Loss: 0.1047\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 540/600, Loss: 0.1067\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 550/600, Loss: 0.1065\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 560/600, Loss: 0.1095\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 570/600, Loss: 0.1133\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 580/600, Loss: 0.1084\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 590/600, Loss: 0.1043\n",
            "Train Layer: 0, Epoch: 2/5, Iter: 600/600, Loss: 0.1091\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 10/600, Loss: 0.1108\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 20/600, Loss: 0.1034\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 30/600, Loss: 0.1032\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 40/600, Loss: 0.1067\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 50/600, Loss: 0.1049\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 60/600, Loss: 0.1057\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 70/600, Loss: 0.1072\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 80/600, Loss: 0.1001\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 90/600, Loss: 0.1027\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 100/600, Loss: 0.0995\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 110/600, Loss: 0.1074\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 120/600, Loss: 0.1000\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 130/600, Loss: 0.0966\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 140/600, Loss: 0.1044\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 150/600, Loss: 0.1044\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 160/600, Loss: 0.1003\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 170/600, Loss: 0.1022\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 180/600, Loss: 0.0969\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 190/600, Loss: 0.1035\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 200/600, Loss: 0.0975\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 210/600, Loss: 0.1000\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 220/600, Loss: 0.0974\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 230/600, Loss: 0.0977\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 240/600, Loss: 0.0994\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 250/600, Loss: 0.0989\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 260/600, Loss: 0.0979\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 270/600, Loss: 0.1001\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 280/600, Loss: 0.1019\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 290/600, Loss: 0.0941\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 300/600, Loss: 0.0956\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 310/600, Loss: 0.0935\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 320/600, Loss: 0.0948\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 330/600, Loss: 0.0936\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 340/600, Loss: 0.0874\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 350/600, Loss: 0.0920\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 360/600, Loss: 0.0933\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 370/600, Loss: 0.0951\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 380/600, Loss: 0.0978\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 390/600, Loss: 0.0973\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 400/600, Loss: 0.0971\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 410/600, Loss: 0.0968\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 420/600, Loss: 0.0912\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 430/600, Loss: 0.0906\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 440/600, Loss: 0.0913\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 450/600, Loss: 0.0911\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 460/600, Loss: 0.0931\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 470/600, Loss: 0.0915\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 480/600, Loss: 0.0899\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 490/600, Loss: 0.0911\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 500/600, Loss: 0.0873\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 510/600, Loss: 0.0924\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 520/600, Loss: 0.0910\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 530/600, Loss: 0.0935\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 540/600, Loss: 0.0875\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 550/600, Loss: 0.0945\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 560/600, Loss: 0.0873\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 570/600, Loss: 0.0916\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 580/600, Loss: 0.0882\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 590/600, Loss: 0.0864\n",
            "Train Layer: 0, Epoch: 3/5, Iter: 600/600, Loss: 0.0894\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 10/600, Loss: 0.0900\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 20/600, Loss: 0.0900\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 30/600, Loss: 0.0906\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 40/600, Loss: 0.0900\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 50/600, Loss: 0.0869\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 60/600, Loss: 0.0875\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 70/600, Loss: 0.0889\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 80/600, Loss: 0.0947\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 90/600, Loss: 0.0918\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 100/600, Loss: 0.0894\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 110/600, Loss: 0.0866\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 120/600, Loss: 0.0866\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 130/600, Loss: 0.0891\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 140/600, Loss: 0.0813\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 150/600, Loss: 0.0894\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 160/600, Loss: 0.0849\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 170/600, Loss: 0.0891\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 180/600, Loss: 0.0857\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 190/600, Loss: 0.0860\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 200/600, Loss: 0.0864\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 210/600, Loss: 0.0905\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 220/600, Loss: 0.0862\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 230/600, Loss: 0.0867\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 240/600, Loss: 0.0902\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 250/600, Loss: 0.0886\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 260/600, Loss: 0.0872\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 270/600, Loss: 0.0881\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 280/600, Loss: 0.0859\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 290/600, Loss: 0.0858\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 300/600, Loss: 0.0889\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 310/600, Loss: 0.0836\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 320/600, Loss: 0.0864\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 330/600, Loss: 0.0826\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 340/600, Loss: 0.0836\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 350/600, Loss: 0.0850\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 360/600, Loss: 0.0808\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 370/600, Loss: 0.0859\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 380/600, Loss: 0.0868\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 390/600, Loss: 0.0822\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 400/600, Loss: 0.0821\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 410/600, Loss: 0.0853\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 420/600, Loss: 0.0819\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 430/600, Loss: 0.0858\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 440/600, Loss: 0.0825\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 450/600, Loss: 0.0807\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 460/600, Loss: 0.0874\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 470/600, Loss: 0.0847\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 480/600, Loss: 0.0851\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 490/600, Loss: 0.0810\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 500/600, Loss: 0.0814\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 510/600, Loss: 0.0834\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 520/600, Loss: 0.0800\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 530/600, Loss: 0.0827\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 540/600, Loss: 0.0853\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 550/600, Loss: 0.0843\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 560/600, Loss: 0.0848\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 570/600, Loss: 0.0824\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 580/600, Loss: 0.0860\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 590/600, Loss: 0.0797\n",
            "Train Layer: 0, Epoch: 4/5, Iter: 600/600, Loss: 0.0855\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 10/600, Loss: 0.0798\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 20/600, Loss: 0.0832\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 30/600, Loss: 0.0810\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 40/600, Loss: 0.0819\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 50/600, Loss: 0.0802\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 60/600, Loss: 0.0820\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 70/600, Loss: 0.0855\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 80/600, Loss: 0.0825\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 90/600, Loss: 0.0842\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 100/600, Loss: 0.0807\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 110/600, Loss: 0.0815\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 120/600, Loss: 0.0794\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 130/600, Loss: 0.0816\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 140/600, Loss: 0.0801\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 150/600, Loss: 0.0814\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 160/600, Loss: 0.0800\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 170/600, Loss: 0.0815\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 180/600, Loss: 0.0808\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 190/600, Loss: 0.0794\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 200/600, Loss: 0.0804\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 210/600, Loss: 0.0837\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 220/600, Loss: 0.0831\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 230/600, Loss: 0.0800\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 240/600, Loss: 0.0791\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 250/600, Loss: 0.0802\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 260/600, Loss: 0.0780\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 270/600, Loss: 0.0787\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 280/600, Loss: 0.0793\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 290/600, Loss: 0.0826\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 300/600, Loss: 0.0760\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 310/600, Loss: 0.0831\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 320/600, Loss: 0.0770\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 330/600, Loss: 0.0776\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 340/600, Loss: 0.0784\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 350/600, Loss: 0.0789\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 360/600, Loss: 0.0800\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 370/600, Loss: 0.0806\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 380/600, Loss: 0.0814\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 390/600, Loss: 0.0785\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 400/600, Loss: 0.0802\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 410/600, Loss: 0.0790\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 420/600, Loss: 0.0787\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 430/600, Loss: 0.0800\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 440/600, Loss: 0.0803\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 450/600, Loss: 0.0780\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 460/600, Loss: 0.0773\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 470/600, Loss: 0.0770\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 480/600, Loss: 0.0762\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 490/600, Loss: 0.0807\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 500/600, Loss: 0.0790\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 510/600, Loss: 0.0788\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 520/600, Loss: 0.0797\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 530/600, Loss: 0.0788\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 540/600, Loss: 0.0782\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 550/600, Loss: 0.0807\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 560/600, Loss: 0.0775\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 570/600, Loss: 0.0802\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 580/600, Loss: 0.0732\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 590/600, Loss: 0.0763\n",
            "Train Layer: 0, Epoch: 5/5, Iter: 600/600, Loss: 0.0778\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 10/600, Loss: 0.6748\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 20/600, Loss: 0.6593\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 30/600, Loss: 0.6490\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 40/600, Loss: 0.6417\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 50/600, Loss: 0.6360\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 60/600, Loss: 0.6339\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 70/600, Loss: 0.6325\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 80/600, Loss: 0.6304\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 90/600, Loss: 0.6297\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 100/600, Loss: 0.6275\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 110/600, Loss: 0.6267\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 120/600, Loss: 0.6257\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 130/600, Loss: 0.6220\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 140/600, Loss: 0.6207\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 150/600, Loss: 0.6178\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 160/600, Loss: 0.6170\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 170/600, Loss: 0.6132\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 180/600, Loss: 0.6110\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 190/600, Loss: 0.6056\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 200/600, Loss: 0.6036\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 210/600, Loss: 0.5993\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 220/600, Loss: 0.6002\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 230/600, Loss: 0.5982\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 240/600, Loss: 0.5940\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 250/600, Loss: 0.5926\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 260/600, Loss: 0.5872\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 270/600, Loss: 0.5835\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 280/600, Loss: 0.5835\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 290/600, Loss: 0.5775\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 300/600, Loss: 0.5790\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 310/600, Loss: 0.5724\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 320/600, Loss: 0.5754\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 330/600, Loss: 0.5702\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 340/600, Loss: 0.5703\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 350/600, Loss: 0.5712\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 360/600, Loss: 0.5676\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 370/600, Loss: 0.5629\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 380/600, Loss: 0.5595\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 390/600, Loss: 0.5565\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 400/600, Loss: 0.5598\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 410/600, Loss: 0.5573\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 420/600, Loss: 0.5537\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 430/600, Loss: 0.5533\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 440/600, Loss: 0.5523\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 450/600, Loss: 0.5506\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 460/600, Loss: 0.5480\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 470/600, Loss: 0.5462\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 480/600, Loss: 0.5425\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 490/600, Loss: 0.5447\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 500/600, Loss: 0.5404\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 510/600, Loss: 0.5363\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 520/600, Loss: 0.5377\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 530/600, Loss: 0.5374\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 540/600, Loss: 0.5320\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 550/600, Loss: 0.5283\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 560/600, Loss: 0.5284\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 570/600, Loss: 0.5302\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 580/600, Loss: 0.5238\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 590/600, Loss: 0.5327\n",
            "Train Layer: 1, Epoch: 1/5, Iter: 600/600, Loss: 0.5262\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-e78c5327f465>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Pre-train each layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnun_layers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_tranin_layer_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Training the network as a whole\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-60e69f749346>\u001b[0m in \u001b[0;36mtrain_layers\u001b[0;34m(layers_list, layer, epoch, validate)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mlayers_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mtest_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./test_images/pretrain_layer_{}_epoch_{}_output.png'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-1f5e5f49a031>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_training_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (100x784 and 256x64)"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if not os.path.exists('test_images'):\n",
        "    os.mkdir('test_images')\n",
        "if not os.path.exists('models'):\n",
        "    os.mkdir('models')\n",
        "\n",
        "nun_layers = 5\n",
        "encoder_1 = AutoEncoderLayer(784, 256, SelfTraining=True)\n",
        "encoder_2 = AutoEncoderLayer(256, 64, SelfTraining=True)\n",
        "decoder_3 = AutoEncoderLayer(64, 256, SelfTraining=True)\n",
        "decoder_4 = AutoEncoderLayer(256, 784, SelfTraining=True)\n",
        "layers_list = [encoder_1, encoder_2, decoder_3, decoder_4]\n",
        "\n",
        "# Pre-train each layer\n",
        "for level in range(nun_layers - 1):\n",
        "    train_layers(layers_list=layers_list, layer=level, epoch=num_tranin_layer_epochs, validate=True)\n",
        "\n",
        "# Training the network as a whole\n",
        "SAE_model = StackedAutoEncoder(layers_list=layers_list)\n",
        "train_whole(model=SAE_model, epoch=num_tranin_whole_epochs, validate=True)\n",
        "\n",
        "# Save the model (refer: https://pytorch.org/docs/master/notes/serialization.html)\n",
        "torch.save(SAE_model, './models/sae_model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JeBE2_M6G5j"
      },
      "outputs": [],
      "source": [
        "# You can later check the saved test images here\n",
        "!cd /root/test_images\n",
        "!ls"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
